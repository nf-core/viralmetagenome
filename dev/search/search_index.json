{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"nf-core/viralmetagenome: Documentation","text":"<p>The nf-core/viralmetagenome documentation is split into the following pages:</p> <ul> <li>Usage</li> <li>An overview of how the pipeline works, how to run it and a description of all of the different command-line flags.</li> <li>Output</li> <li>An overview of the different results produced by the pipeline and how to interpret them.</li> </ul> <p>You can find a lot more documentation about installing, configuring and running nf-core pipelines on the website: https://nf-co.re</p>"},{"location":"output/","title":"Output","text":""},{"location":"output/#introduction","title":"Introduction","text":"<p>This document describes the output produced by the pipeline. Most of the plots are taken from the MultiQC report, which summarizes results at the end of the pipeline.</p> <p>The directories listed below will be created in the results directory after the pipeline has finished. All paths are relative to the top-level results directory.</p> <p>:::tip A global, partly random prefix can be created using the argument <code>--prefix &lt;string&gt;</code>. The following string will then be used as a prefix to all output files: <code>java     \"&lt;prefix_string&gt;_&lt;date&gt;_&lt;pipeline_version&gt;_&lt;workflow_runName&gt;\"</code> :::</p>"},{"location":"output/#preprocessing","title":"Preprocessing","text":"<p>All output files of the preprocessing steps can be found in the directory <code>preprocessing/</code>.</p>"},{"location":"output/#fastqc","title":"FastQC","text":"<p>:::note{title=\"Output files\" collapse}</p> <ul> <li><code>fastqc/{raw,trim,host}</code></li> <li><code>*_fastqc.html</code>: FastQC report containing quality metrics.</li> <li><code>*_fastqc.zip</code>: Zip archive containing the FastQC report, tab-delimited data file, and plot images.</li> </ul> <p>:::</p> <p>FastQC gives general quality metrics about your sequenced reads. It provides information about the quality score distribution across your reads, per base sequence content (%A/T/G/C), adapter contamination, and overrepresented sequences. For further reading and documentation see the FastQC help pages.</p> <p></p> <p></p> <p></p> <p>:::tip The FastQC plots displayed in the MultiQC report show untrimmed, trimmed, and host filtered reads. Make sure to check the section titles for the correct set of reads. :::</p>"},{"location":"output/#fastp","title":"fastp","text":"<p>fastp is a FASTQ pre-processing tool for quality control, trimming of adapters, quality filtering, and other features.</p> <p>:::note{title=\"Output files\" collapse}</p> <ul> <li><code>fastp/</code></li> <li><code>report/&lt;sample-id&gt;.*{html,json}</code>: report files in different formats.</li> <li><code>log/&lt;sample-id&gt;.*{html,json}</code>: log files.</li> <li><code>&lt;sample-id&gt;.fastp.fastq.gz</code>: file with the trimmed fastq reads.</li> <li><code>fail/&lt;sample-id&gt;.fail.fastq.gz</code>: file with reads that didn't suffice quality controls.</li> </ul> <p>:::</p> <p>By default, viralmetagenome will only provide the report and log files if fastp is selected. The trimmed reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'trimming'</code>. Similarly, the saving of the output reads can be enabled with <code>--save_trimmed_fail</code>.</p>"},{"location":"output/#trimmomatic","title":"Trimmomatic","text":"<p>Trimmomatic is a FASTQ pre-processing tool for quality control, trimming of adapters, quality filtering, and other features.</p> <p>:::note{title=\"Output files\" collapse}</p> <ul> <li><code>trimmomatic/</code></li> <li><code>&lt;sample-id&gt;.fastq.gz</code>: file with the trimmed fastq reads.</li> <li><code>log/&lt;sample-id&gt;.*{html,txt,zip}</code>: log files generated by Trimmomatic.</li> </ul> <p>:::</p> <p>By default, viralmetagenome will only provide the report and log files if Trimmomatic is selected. The trimmed reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'trimming'</code>.</p>"},{"location":"output/#umi-deduplication","title":"UMI-deduplication","text":"<p>UMI-deduplication can be done at the read level using <code>HUMID</code>. Viralmetagenome also provides the opportunity to extract the UMI from the read using <code>UMI-tools extract</code> if the UMI is not in the header. Results will be stored in the <code>preprocessing/umi</code> directory.</p> <p>:::note{title=\"Output files\" collapse}</p> <ul> <li><code>umi/</code></li> <li><code>humid/</code></li> <li><code>log/&lt;sample-id&gt;.log</code>: log file of HUMID.</li> <li><code>annotated/&lt;sample-id&gt;_annotated_*.fastq.gz</code>: annotated FastQ files, reads will have their assigned cluster in the read header.</li> <li><code>deduplicated/&lt;sample-id&gt;_deduplicated_*.fastq.gz</code>: deduplicated FastQ files.</li> <li><code>umitools/</code></li> <li><code>log/&lt;sample-id&gt;.log</code>: log file of UMI-tools.</li> <li><code>extracts/&lt;sample-id&gt;.umi_extract*.fastq.gz</code>: fastq file where UMIs have been removed from the read and moved to the read header.</li> </ul> <p>:::</p> <p>By default, viralmetagenome will not assume reads have UMIs. To enable this, use the parameter <code>--with_umi</code>. Specify where UMI deduplication should occur with <code>--umi_deduplicate</code> if at a <code>read</code> level, on a <code>mapping</code> level, or <code>both</code> at a read and mapping level. The deduplicated reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'deduplication'</code>.</p>"},{"location":"output/#bbduk","title":"BBDuk","text":"<p>BBDuk stands for Decontamination Using Kmers. BBDuk was developed to combine most common data-quality-related trimming, filtering, and masking operations into a single high-performance tool.</p> <p>It is used in viralmetagenome for complexity filtering using different algorithms. This means that it will remove reads with low sequence diversity (e.g., mono- or dinucleotide repeats).</p> <p>:::note{title=\"Output files\" collapse}</p> <ul> <li><code>bbduk/</code></li> <li><code>log/&lt;sample-id&gt;.bbduk.log</code>: log file containing filtering statistics.</li> <li><code>&lt;sample-id&gt;.fastq.gz</code>: resulting FASTQ file without low-complexity reads.</li> </ul> <p>:::</p> <p>By default, viralmetagenome will only provide the log files of BBDuk. The filtered reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'complexity'</code>.</p>"},{"location":"output/#prinseq","title":"prinseq++","text":"<p><code>prinseq++</code> is used in viralmetagenome for complexity filtering using different algorithms. This means that it will remove reads with low sequence diversity (e.g., mono- or dinucleotide repeats).</p> <p>:::note{title=\"Output files\" collapse}</p> <ul> <li><code>prinseq/</code></li> <li><code>log/&lt;sample-id&gt;.log</code>: log file containing filtering statistics.</li> <li><code>&lt;sample-id&gt;.fastq.gz</code>: resulting FASTQ file without low-complexity reads.</li> </ul> <p>:::</p> <p>By default, viralmetagenome will only provide the log files of prinseq. The filtered reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'complexity'</code>.</p>"},{"location":"output/#hostremoval-kraken2","title":"Hostremoval-Kraken2","text":"<p>Kraken2 is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.</p> <p>:::note{title=\"Output files\" collapse}</p> <ul> <li><code>hostremoval-kraken2/</code></li> <li><code>&lt;sample-id&gt;_kraken2_host.report.txt</code>: A profile of the aligned reads to a given host contamination database.</li> <li><code>&lt;sample-id&gt;_kraken2_host.unclassified*.fastq.gz</code>: resulting FASTQ file with reads that don't have any matches to the given host contamination database.</li> </ul> <p>:::</p> <p>By default, viralmetagenome will only provide the log files of Kraken2 which are visualized in MultiQC. The filtered reads can be saved by specifying <code>--save_intermediate_reads</code> or <code>--save_final_reads 'host'</code>.</p>"},{"location":"output/#metagenomic-diversity","title":"Metagenomic Diversity","text":"<p>The results of the metagenomic diversity analysis are stored in the directory <code>metagenomic_diversity/</code>. Results are also visualized in the MultiQC report. </p>"},{"location":"output/#kraken2","title":"Kraken2","text":"<p>Kraken2 is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.</p> <p>:::note{title=\"Output files\" collapse}</p> <ul> <li><code>metagenomic_diversity/kraken2/</code></li> <li><code>&lt;sample-id&gt;.report.txt</code>: A Kraken2 report that summarizes the fraction abundance, taxonomic ID, number of k-mers, taxonomic path of all the hits in the Kraken2 run for a given sample. Will be 6 columns rather than 8 if <code>--save_minimizers</code> specified.</li> <li><code>&lt;sample-id&gt;_kraken2_host.unclassified*.fastq.gz</code>: resulting FASTQ file with reads that don't have any matches to the given host contamination database.</li> <li><code>&lt;sample-id&gt;.classified.fastq.gz</code>: FASTQ file containing all reads that had a hit against a reference in the database for a given sample.</li> <li><code>&lt;sample-id&gt;.unclassified.fastq.gz</code>: FASTQ file containing all reads that did not have a hit in the database for a given sample.</li> <li><code>&lt;sample-id&gt;.classifiedreads.txt</code>: A list of read IDs and the hits each read had against the database for a given sample.</li> </ul> <p>:::</p> <p>By default, viralmetagenome will provide any classified or unclassified fastq files, specify this with <code>--kraken2_save_reads</code>. Similarly, for the classified reads table, specify this with <code>--kraken2_save_readclassification</code>.</p>"},{"location":"output/#kaiju","title":"Kaiju","text":"<p>Kaiju is a program for sensitive taxonomic classification of high-throughput sequencing reads from metagenomic data. It is based on the Burrows-Wheeler transform and the lowest common ancestor algorithm.</p> <p>:::note{title=\"Output files\" collapse}</p> <ul> <li><code>metagenomic_diversity/kaiju/</code></li> <li><code>&lt;sample-id&gt;.tsv</code>: Raw output from Kaiju with taxonomic rank, read ID, and taxonomic ID.</li> <li><code>&lt;sample-id&gt;.txt</code>: A summary of the taxonomic classification of the reads in the sample.</li> </ul> <p>:::</p>"},{"location":"output/#krona","title":"Krona","text":"<p>Krona is a hierarchical data visualization tool that can be used to visualize the taxonomic classification of metagenomic data.</p> <p>:::note{title=\"Output files\" collapse}</p> <ul> <li><code>metagenomic_diversity/krona/</code></li> <li><code>&lt;kaiju|kraken2&gt;_.html</code>: A HTML file containing the Krona visualization of the taxonomic classification of the reads in the sample.</li> </ul> <p>:::</p> <p></p>"},{"location":"output/#assembly-polishing","title":"Assembly &amp; Polishing","text":"<p>The results of the assembly processes &amp; polishing are stored in the directory <code>assembly/</code>.</p> <p>Multiple intermediate files can be generated during the assembly process, some of them might not always be interesting to have. For this reason, there is an option to save the intermediate files with the <code>--save_intermediate_polishing</code> argument which is by default off.</p>"},{"location":"output/#assemblers","title":"Assemblers","text":"<p>Multiple assemblers [spades, trinity, megahit] can be used which have their results combined. Each assembler has its own directory in the <code>assembly/assemblers</code> directory, where there will be a subfolder for the contigs and the QC results from QUAST.</p> <p>:::note{title=\"Output files\" collapse}</p> <ul> <li><code>assemblers/</code></li> <li><code>spades/&lt;spades_mode&gt;/</code><ul> <li><code>contigs/&lt;sample-id&gt;_spades.fa.gz</code>: Contigs generated by SPAdes.</li> <li><code>log/&lt;sample-id&gt;_spades.log</code>: Directory containing the log file of the SPAdes run.</li> </ul> </li> <li><code>quast/&lt;sample-id&gt;_spades.tsv</code>: Directory containing the QUAST report. - <code>trinity/</code> - <code>contigs/&lt;sample-id&gt;_trinity.fa.gz</code>: Contigs generated by Trinity. - <code>quast/&lt;sample-id&gt;_trinity.tsv</code>: Directory containing the QUAST report - <code>megahit/</code> - <code>contigs/&lt;sample-id&gt;_megahit.fa.gz</code>: Contigs generated by Megahit. - <code>quast/&lt;sample-id&gt;_megahit.tsv</code>: Directory containing the QUAST report.</li> </ul> <p>:::</p> <p>QUAST results are also summarized and plotted in the MultiQC report.</p> <p></p> <p>Finally, the results of the assemblers are combined and stored in the <code>tools_combined/</code> directory.</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>assemblers</code> - <code>tools_combined/&lt;sample-id&gt;.combined.fa</code>: Contigs generated by combining the results of the assemblers.</li> </ul> <p>:::</p>"},{"location":"output/#sspace-basic","title":"SSPACE Basic","text":"<p>SSPACE Basic is a tool for scaffolding contigs using paired-end reads. It is modified from the SSAKE assembler and has the feature of extending contigs using reads that are unmappable in the contig assembly step.</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>sspace_basic/</code> - <code>scaffolds/&lt;sample-id&gt;.scaffolds.fasta</code>: Scaffolds generated by SSPACE Basic. - <code>log/&lt;sample-id&gt;.*.txt</code>: Various txt files containing log and summary information on the SSPACE Basic run.</li> </ul> <p>:::</p>"},{"location":"output/#prinseq-contigs","title":"prinseq++ - contigs","text":"<p><code>prinseq++</code> is used for complexity filtering of contigs.</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>prinseq/</code></li> <li><code>scaffolds/&lt;sample-id&gt;.scaffolds.fasta</code>: Scaffolds generated by SSPACE Basic.</li> <li><code>log/&lt;sample-id&gt;.*.txt</code>: Various txt files containing log and summary information on the SSPACE Basic run.</li> </ul> <p>:::</p>"},{"location":"output/#blast","title":"BLAST","text":"<p>BLAST is a sequence comparison tool that can be used to compare a query sequence against a database of sequences. In viralmetagenome, BLAST is used to compare the contigs generated by the assemblers to a database of viral sequences.</p> <p>By default, viralmetagenome will only provide the BLAST results in a tabular format. It will have selected only for the top five hits and will also have a filtered version where it will only include hits with an e-value of 0.01 or lower, a bitscore of 50 or higher, and an alignment percentage of 0.80 or higher.</p> <p>:::info{title=\"Column names\" collapse}</p> <ul> <li>qseqid</li> <li>sseqid</li> <li>stitle</li> <li>pident</li> <li>qlen</li> <li>slen</li> <li>length</li> <li>mismatch</li> <li>gapopen</li> <li>qstart</li> <li>qend</li> <li>sstart</li> <li>send</li> <li>evalue</li> <li>bitscore</li> </ul> <p>:::</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>polishing/</code></li> <li><code>blast/&lt;sample-id&gt;_filter.tsv</code>: Filtered BLAST results in tabular format.</li> <li><code>intermediate/blast/filtered-sequences/&lt;sample-id&gt;_withref.fa</code>: Contigs with the blast hit sequence in a fasta file.</li> <li><code>intermediate/blast/hits/&lt;sample-id&gt;.txt</code>: unfiltered BLAST results in tabular format.</li> </ul> <p>:::</p> <p>By default, viralmetagenome will only provide the filtered blast.txt file. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p>"},{"location":"output/#preclustering-kaiju-kraken2","title":"Preclustering - Kaiju &amp; Kraken2","text":"<p>Kaiju is a program for sensitive taxonomic classification of high-throughput sequencing reads from metagenomic data. It is based on the Burrows-Wheeler transform and the lowest common ancestor algorithm.</p> <p>Kraken2 is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>polishing/intermediate/precluster</code></li> <li><code>kaiju/&lt;sample-id&gt;_kaiju.tsv</code>: Raw output from Kaiju with taxonomic rank, read ID, and taxonomic ID.</li> <li><code>kraken2/&lt;sample-id&gt;_kraken2_reports.txt</code>: A Kraken2 report that summarizes the fraction abundance, taxonomic ID, number of k-mers, taxonomic path of all the hits in the Kraken2 run for a given sample. Will be 6 columns rather than 8 if <code>--save_minimizers</code> specified.</li> <li><code>kraken2/&lt;sample-id&gt;_kraken2.classifiedreads.txt</code>: A list of read IDs and the hits each read had against the database for a given sample.</li> <li><code>merged_classifications/&lt;sample-id&gt;.txt</code>: Taxonomy merged based on the specified strategy, filtered based on specified filters, and simplified up to a certain taxonomy with the columns being taxonomic rank, read ID, and taxonomic ID.</li> <li><code>sequences/&lt;sample-id&gt;/&lt;sample-id&gt;_taxid&lt;taxonomic ID&gt;.fa</code>: Fasta file with the contigs that were classified to that specific taxonomic ID.</li> </ul> <p>:::</p> <p>By default, viralmetagenome will not provide any preclustering files. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p>"},{"location":"output/#clustering","title":"Clustering","text":"<p>The output files of each clustering method are directly put in the <code>assembly/polishing</code> directory, with the exception of a summary file that is generated by the pipeline for each cluster with the size of the cluster, the centroid, etc.</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>polishing/intermediate/cluster/</code></li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.summary_mqc.tsv</code>: A tabular file with comments used for MultiQC with statistics on the number of identified clusters in a sample.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.clusters.tsv</code>: A tabular file with metadata on all clusters in a sample. It's the JSON file of all clusters in a table format.</li> </ul> <p>:::</p> <p>:::tip Whenever there is a 'cl#' in the file name, it refers to the cluster number of that sample. :::</p> <p>By default, viralmetagenome will not provide any clustering overview files. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p>"},{"location":"output/#cd-hit-est","title":"CD-HIT-EST","text":"<p>CD-HIT is a very fast, widely used program for clustering and comparing protein or nucleotide sequences.</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>polishing/cdhit/</code></li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.fa.clstr</code>: A cluster file containing the clustering information. where \"&gt;\" starts a new cluster, a \"*\" at the end means that this sequence is the representative or centroid of this cluster, and a \"%\" is the identity between this sequence and the representative.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.fa</code>: A fasta file containing the centroid sequence.</li> </ul> <p>:::</p>"},{"location":"output/#vsearch-cluster","title":"vsearch-cluster","text":"<p>vsearch implements a single-pass, greedy centroid-based clustering algorithm, similar to the algorithms implemented in usearch, DNAclust, and sumaclust for example. The output has to be in the <code>--uc</code> format or else the pipeline will not be able to process the output.</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>polishing/vsearch/</code></li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;.tsv.gz</code>: A cluster file containing the clustering information.</li> </ul> <p>:::</p> <p>:::info{title=\"vsearch -uc columns\" collapse}</p> <ol> <li>Entry (S, H, or C):</li> <li>Record type: S, H, or C.</li> <li>Cluster number (zero-based).</li> <li>Centroid length (S), query length (H), or cluster size (C).</li> <li>Percentage of similarity with the centroid sequence (H), or set to \u2019*\u2019 (S, C).</li> <li>Match orientation + or - (H), or set to \u2019\u2019 (S, C). Not used, always set to \u2019\u2019 (S, C) or to zero (H).</li> <li>Not used, always set to \u2019*\u2019 (S, C) or to zero (H).</li> <li>Set to \u2019*\u2019 (S, C) or, for H, compact representation of the pairwise alignment using the CIGAR format (Compact Idiosyncratic Gapped Alignment Report): M (match/mismatch), D (deletion), and I (insertion). The equal sign \u2019=\u2019 indicates that the query is identical to the centroid sequence.</li> <li>Label of the query sequence (H), or of the centroid sequence (S, C). 10. Label of the centroid sequence (H), or set to \u2019*\u2019 (S, C).</li> </ol> <p>:::</p>"},{"location":"output/#mmseqs2","title":"MMseqs2","text":"<p>MMseqs2 is a software suite to search and cluster huge protein and nucleotide sequence sets. The cascaded clustering workflow (<code>mmseqs-cluster</code>) first runs linclust, the linear-time clustering module of mmseqs (<code>mmseqs-linclust</code>), that can produce clustering\u2019s down to 50% sequence identity in very short time.</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>polishing/</code></li> <li><code>mmseqs2/&lt;sample-id&gt;/&lt;sample-id&gt;.tsv</code>: A cluster file containing the clustering information. Where the first column is the cluster representative and the second column the member.</li> <li><code>intermediate/mmseqs/clustered_db/&lt;sample-id&gt;*</code>: A MMseqs2 database of the clustered sequences.</li> <li><code>intermediate/mmseqs/sequence_db/&lt;sample-id&gt;*</code>: A MMseqs2 database of the input sequences (contigs + blast hits).</li> </ul> <p>:::</p>"},{"location":"output/#vrhyme","title":"vRhyme","text":"<p>vRhyme is a multi-functional tool for binning virus genomes from metagenomes. vRhyme functions by utilizing coverage variance comparisons and supervised machine learning classification of sequence features to construct viral metagenome-assembled genomes (vMAGs).</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>polishing/vrhyme</code></li> <li><code>&lt;sample-id&gt;/vRhyme_best_bins.#.membership.tsv</code>: scaffold membership of best bins.</li> <li><code>&lt;sample-id&gt;/vRhyme_best_bins.#.summary.tsv</code>: summary stats of best bins.</li> </ul> <p>:::</p>"},{"location":"output/#mash","title":"Mash","text":"<p>Mash calculates the distance between two sequences based on the jaccard distance. The Mash distance can be quickly computed from the size-reduced sketches alone, yet produces a result that strongly correlates with alignment-based measures such as the Average Nucleotide Identity (ANI).</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>polishing/mash</code></li> <li><code>&lt;sample-id&gt;/dist/*.tsv</code>: A distance matrix of the genomes with ANI.</li> <li><code>&lt;sample-id&gt;/cluster/*.tsv</code>: A table where the first column represents the contig/genome and the second column it's corresponding cluster.</li> <li><code>&lt;sample-id&gt;/visual/*.png</code>: A visualization of the network.</li> </ul> <p>:::</p> <p>The network of a triple segmented Hazara virus looks like this, each node represents a contig colored on cluster. The edge represents that the ANI is higher than the specified <code>--identity_threshold</code>.</p> <p></p> <p>:::info{title=\"What are those names?\"} Most assemblers tend to give each contig name a specific prefix. For example,</p> <ul> <li>Trinity: <code>'TRINITY_...'</code></li> <li>SPAdes: <code>'NODE_...'</code></li> <li>Megahit: <code>'k\\d{3}_...'</code></li> </ul> <p>:::</p> <p>Based on these prefixes viralmetagenome separates external references from denovo contigs. If any assemblers are added, consider specifying a specific regex for <code>--assembler_patterns</code>.</p>"},{"location":"output/#minimap2","title":"Minimap2","text":"<p>Minimap2 is a versatile sequence alignment program that aligns larger DNA or mRNA sequences against a large reference database.</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>polishing/scaffolding/&lt;sample-id&gt;/minimap</code></li> <li><code>&lt;sample-id&gt;_cl#.bam</code>: A BAM file containing the alignment of contigs to the centroid.</li> <li><code>&lt;sample-id&gt;_cl#.mmi</code>: The centroid index file.</li> </ul> <p>:::</p> <p>By default, viralmetagenome will not provide the minimap output files. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p>"},{"location":"output/#ivar-contig-consensus","title":"iVar contig consensus","text":"<p>iVar is a computational method for calling consensus sequences from viral populations.</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>polishing/scaffolding/&lt;sample-id&gt;</code></li> <li><code>&lt;sample-id&gt;_cl#_consensus.fa</code>: A fasta file containing the consensus sequence of the cluster.</li> <li><code>&lt;sample-id&gt;_cl#_consensus.mpileup</code>: A mpileup file containing depth at each position of the consensus sequence.</li> <li><code>hybrid-&lt;sample-id&gt;_cl#_consensus.fa</code>: A fasta file containing the hybrid consensus sequence of the cluster and the reference.</li> <li><code>/visualised/</code><ul> <li><code>*.png</code>: A visualization of the consensus sequence displaying which regions came from the reference and which from the contigs.</li> <li><code>*.txt</code>: The alignment of the reference to the consensus sequence written as a blast alignment.</li> </ul> </li> </ul> <p>:::</p> <p>By default, viralmetagenome will not provide the iVar output files. The intermediate files can be saved by specifying <code>--save_intermediate_polishing</code>.</p> <p>A visualization is made to show which regions came from the external reference (red) and which from the denovo contigs (green). For example,</p> <p></p> <p>:::info The hybrid consensus is generated by mapping the contigs to the reference and then calling the consensus sequence. This is done to fill in the gaps in the contigs with the reference sequence, if there are no positions with 0 coverage there will not be a hybrid consensus and the output from iVar will be used. :::</p>"},{"location":"output/#variant-calling-iterative-refinement","title":"Variant Calling &amp; Iterative Refinement","text":"<p>The results from variant calling, resulting from the mapping constraints &amp; the final round of polishing are stored in the directory <code>variant_calling/</code>.</p> <p>:::info Mapping constraints are combined with the specified samples, here, the identifier of the mapping constraint combined with the sample identifier. All results will have a new prefix which is <code>&lt;sample-id&gt;_&lt;mapping_constraint_id&gt;-CONSTRAINT</code>. :::</p> <p>The results from the iterations are stored with the same structure as the final round of polishing in the <code>assembly/polishing/iterations/it#</code> directory.</p> <p>:::info To be able to make a distinction between the output files of the iterations, viralmetagenome follows a schema where it starts from <code>singletons</code> or a <code>consensus</code> goes through the iterations and ends with the <code>variant-calling</code>. The output files will have the following structure:</p> <pre><code>graph LR\n    F[singleton] --&gt; B[Iteration 1: 'it1']\n    A[consensus] --&gt; B[Iteration 1: 'it1']\n    B --&gt; C[Iteration 2: 'it2']\n    C --&gt; D[...]\n    D --&gt; E[Variant-calling: 'itvariant-calling']</code></pre> <p>The prefix of the sample is combined with the previous state of the sample. For example, in the first iteration (directory <code>iterations/it1</code>), reads will be mapped to the reference-assisted de novo consensus sequence (ie <code>consensus</code>) and so the output file will be <code>assembly/polishing/iterations/it1/bwamem2/bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_consensus.bam</code>. :::</p>"},{"location":"output/#reference-selection","title":"Reference selection","text":"<p>The reference selection is done using <code>mash</code> tool. Here the reference file is sketched (<code>variants/mapping-info/mash/sketch</code>) and compared to the reads (<code>variants/mapping-info/mash/screen</code>) where the reference with the highest estimated average nucleotide identity (ANI) and shared hashes is selected (<code>variants/mapping-info/mash/select-ref</code>).</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>variants/mapping-info/mash</code></li> <li><code>sketch/&lt;sample-id&gt;_&lt;constraint-id&gt;.msh</code>: The sketch file of the reads.</li> <li><code>screen/&lt;sample-id&gt;_&lt;constraint-id&gt;.screen</code>: The tab results file of the comparisons between references and reads.</li> <li><code>select-ref/&lt;sample-id&gt;_&lt;constraint-id&gt;.json</code>: The reference with the highest estimated ANI and shared hashes.</li> </ul> <p>:::</p> <p>:::info{title=\"Column names: mash-screen\" collapse}</p> <ul> <li>identity</li> <li>shared-hashes</li> <li>median-multiplicity</li> <li>p-value</li> <li>query-ID</li> <li>query-comment</li> </ul> <p>:::</p>"},{"location":"output/#read-mapping","title":"Read mapping","text":"<p>The mapping results are stored in the directory <code>variants/mapping-info/</code> or in the iterations directory <code>assembly/polishing/iterations/it#</code>.</p> <p>If bowtie is used, the output from the raw mapping results (in addition to the results after deduplication) are included in the multiqc report.</p> <p></p> <p>:::abstract{title=\"Output files - variants\" collapse}</p> <ul> <li><code>variants/mapping-info/</code></li> <li><code>bwamem2/</code><ul> <li><code>index/&lt;sample-id&gt;_&lt;constraint-id&gt;/*</code>: The index files of the consensus.</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.fastq.gz</code>: A fastq file containing the unmapped reads.</li> </ul> </li> <li><code>bwamem/</code><ul> <li><code>index/&lt;sample-id&gt;_&lt;constraint-id&gt;/*</code>: The index files of the consensus.</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.fastq.gz</code>: A fastq file containing the unmapped reads.</li> </ul> </li> <li><code>bowtie2/</code><ul> <li><code>build/&lt;sample-id&gt;_&lt;constraint-id&gt;/*</code>: The index files of the consensus.</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.fastq.gz</code>: A fastq file containing the unmapped reads.</li> <li><code>log/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.log</code>: A log file of the bowtie2 run.</li> </ul> </li> </ul> <p>:::</p> <p>:::abstract{title=\"Output files - iterations\" collapse}</p> <ul> <li><code>assembly/polishing/iterations/it#/</code></li> <li><code>bwamem2/</code><ul> <li><code>index/&lt;sample-id&gt;/*</code>: The index files of the consensus.</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.fastq.gz</code>: A fastq file containing the unmapped reads.</li> </ul> </li> <li><code>bwamem/</code><ul> <li><code>index/&lt;sample-id&gt;/*</code>: The index files of the consensus.</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.fastq.gz</code>: A fastq file containing the unmapped reads.</li> </ul> </li> <li><code>bowtie2/</code><ul> <li><code>build/&lt;sample-id&gt;/*</code>: The index files of the consensus.</li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>unmapped/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.fastq.gz</code>: A fastq file containing the unmapped reads.</li> <li><code>log/&lt;sample-id&gt;_cl#_it#.log</code>: A log file of the bowtie2 run.</li> </ul> </li> </ul> <p>:::</p>"},{"location":"output/#deduplication","title":"Deduplication","text":"<p>To accommodate for PCR duplicates, the reads are deduplicated. The deduplication results are stored in the directory <code>variants/mapping-info/deduplicate/</code> or in the iterations directory <code>assembly/polishing/iterations/it#/deduplicate</code>.</p> <p>Deduplication results are also visualized within the MultiQC report.</p>"},{"location":"output/#umi-tools","title":"UMI-tools","text":"<p><code>UMI-tools</code> is a set of tools for handling Unique Molecular Identifiers (UMIs) in NGS data. The deduplication is done by the <code>dedup</code> tool.</p> <p>Number of deduplicated reads: </p> <p>Summary statistics: </p> <p>:::abstract{title=\"Output files - variants\" collapse}</p> <ul> <li><code>variants/mapping-info/deduplicate/</code></li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.umi_deduplicated.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>log/</code><ul> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.umi_deduplicated.log</code>: A log file of the UMI-tools run.</li> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.umi_deduplicated_edit_distance.tsv</code>: Reports the (binned) average edit distance between the UMIs at each position.</li> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.umi_deduplicated_per_umi.tsv</code>: UMI-level summary statistics.</li> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.umi_deduplicated_per_umi_per_position.tsv</code>: Tabulates the counts for unique combinations of UMI and position.</li> </ul> </li> </ul> <p>:::</p> <p>:::abstract{title=\"Output files - iterations\" collapse}</p> <ul> <li><code>assembly/polishing/iterations/it#/deduplicate</code></li> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.umi_deduplicated.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>log/</code><ul> <li><code>&lt;sample-id&gt;_cl#_it#.umi_deduplicated.log</code>: A log file of the UMI-tools run.</li> <li><code>&lt;sample-id&gt;_cl#_it#.umi_deduplicated_edit_distance.tsv</code>: Reports the (binned) average edit distance between the UMIs at each position.</li> <li><code>&lt;sample-id&gt;_cl#_it#.umi_deduplicated_per_umi.tsv</code>: UMI-level summary statistics.</li> <li><code>&lt;sample-id&gt;_cl#_it#.umi_deduplicated_per_umi_per_position.tsv</code>: Tabulates the counts for unique combinations of UMI and position.</li> </ul> </li> </ul> <p>:::</p>"},{"location":"output/#picard-mark-duplicates","title":"Picard - Mark Duplicates","text":"<p><code>Picard</code> is a set of command line tools for manipulating high-throughput sequencing data and formats such as SAM/BAM/CRAM and VCF. The deduplication is done by the <code>MarkDuplicates</code> tool.</p> <p></p> <p>:::abstract{title=\"Output files - variants\" collapse}</p> <ul> <li><code>variants/mapping-info/deduplicate/</code></li> <li><code>picard/</code><ul> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>log/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.dedup.MarkDuplicates.metrics.txt</code>: Deduplication metrics from Picard.</li> </ul> </li> </ul> <p>:::</p> <p>:::abstract{title=\"Output files - iterations\" collapse}</p> <ul> <li><code>assembly/polishing/iterations/it#/deduplicate</code></li> <li><code>picard/</code><ul> <li><code>bam/&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.bam</code>: A BAM file containing the alignment of contigs to the consensus.</li> <li><code>log/&lt;sample-id&gt;_cl#_it#.dedup.MarkDuplicates.metrics.txt</code>: Deduplication metrics from Picard.</li> </ul> </li> </ul> <p>:::</p>"},{"location":"output/#mapping-statistics","title":"Mapping statistics","text":"<p>:::info If <code>--deduplicate</code> is set to <code>true</code> [default], all metrics will be calculated on the deduplicated bam file. :::</p>"},{"location":"output/#samtools","title":"Samtools","text":"<p>Samtools is a suite of programs for interacting with high-throughput sequencing data. We use samtools in this pipeline to obtain mapping statistics from three tools: <code>flagstat</code>, <code>idxstats</code> and <code>stats</code>.</p> <p> </p> <p>:::abstract{title=\"Output files - variants\" collapse}</p> <ul> <li><code>variants/mapping-info/metrics</code></li> <li><code>flagstat/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.flagstat</code>: A text file containing the flagstat output.</li> <li><code>idxstats/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.idxstats</code>: A text file containing the idxstats output.</li> <li><code>stats/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.stats</code>: A text file containing the stats output.</li> </ul> <p>:::</p> <p>:::abstract{title=\"Output files - iterations\" collapse}</p> <ul> <li><code>assembly/polishing/iterations/it#/metrics</code></li> <li><code>flagstat/&lt;sample-id&gt;_cl#_it#.flagstat</code>: A text file containing the flagstat output.</li> <li><code>idxstats/&lt;sample-id&gt;_cl#_it#.idxstats</code>: A text file containing the idxstats output.</li> <li><code>stats/&lt;sample-id&gt;_cl#_it#.stats</code>: A text file containing the stats output.</li> </ul> <p>:::</p>"},{"location":"output/#picard-collect-multiple-metrics","title":"Picard - Collect Multiple Metrics","text":"<p><code>Picard</code> is a set of command line tools for manipulating high-throughput sequencing data. We use picard-tools in this pipeline to obtain mapping and coverage metrics.</p> <p>:::abstract{title=\"Output files - variants\" collapse}</p> <ul> <li><code>variants/mapping-info/metrics/picard</code></li> <li><code>*.CollectMultipleMetrics.*</code>: Alignment QC files from picard CollectMultipleMetrics in <code>*_metrics</code> textual format.</li> <li><code>*.pdf</code> plots for metrics obtained from CollectMultipleMetrics.</li> </ul> <p>:::</p> <p>:::abstract{title=\"Output files - iterations\" collapse}</p> <ul> <li><code>assembly/polishing/iterations/it#/metrics/picard</code></li> <li><code>*.CollectMultipleMetrics.*</code>: Alignment QC files from picard CollectMultipleMetrics in <code>*_metrics</code> textual format.</li> <li><code>*.pdf</code> plots for metrics obtained from CollectMultipleMetrics.</li> </ul> <p>:::</p>"},{"location":"output/#custom-mpileup-like-file","title":"Custom - mpileup like file","text":"<p>To facilitate the intra host analysis, a mpileup like file is generated. This file contains the depth of every nucleotide at each position of the reference as well as the shannon entropy and a weighted shannon entropy based on the following formulae.</p> <ul> <li>Shannon entropy: $$ H = -\\sum_{i=1}^4 p_i \\ln p_i $$</li> <li>Weighted Shannon entropy: $$ w(H) = \\frac{N}{N+k} \\cdot H $$</li> </ul> <p>Where \\(N\\) is the total bases at a position, \\(k\\) is the pseudocount (default 50), and \\(p_i\\) is the frequency of the nucleotide \\(i\\).</p> <p>:::abstract{title=\"Output files - variants\" collapse}</p> <ul> <li><code>variants/mapping-info/custom-vcf/&lt;sample-id&gt;</code></li> <li><code>*.tsv</code>: A custom tsv file containing the depth of every nucleotide at each position of the reference.</li> </ul> <p>:::</p> <p>:::abstract{title=\"Output files - iterations\" collapse}</p> <ul> <li><code>assembly/polishing/iterations/it#/custom-vcf/&lt;sample-id&gt;</code></li> <li><code>*.tsv</code>: A custom tsv file containing the depth of every nucleotide at each position of the reference.</li> </ul> <p>:::</p>"},{"location":"output/#mosdepth-coverage","title":"Mosdepth - Coverage","text":"<p>mosdepth is a fast BAM/CRAM depth calculation for WGS, exome, or targeted sequencing. mosdepth is used in this pipeline to obtain genome-wide coverage values in 200bp windows. The results are rendered in MultiQC (genome-wide coverage).</p> <p> </p> <p>:::abstract{title=\"Output files - variants\" collapse}</p> <ul> <li><code>variants/mapping-info/metrics/mosdepth</code></li> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.per-base.bed.gz</code>: A bed file containing the coverage values in 200bp windows.</li> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.per-base.bed.gz.csi</code>: Indexed bed file.</li> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.mosdepth.summary.txt</code>: Summary metrics including mean, min and max coverage values.</li> <li><code>&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.mosdepth.global.dist.txt</code>: A cumulative distribution indicating the proportion of total bases that were covered for at least a given coverage value.</li> </ul> <p>:::</p> <p>:::abstract{title=\"Output files - iterations\" collapse}</p> <ul> <li><code>assembly/polishing/iterations/it#/metrics/mosdepth</code></li> <li><code>&lt;sample-id&gt;_cl#_it#.per-base.bed.gz</code>: A bed file containing the coverage values in 200bp windows.</li> <li><code>&lt;sample-id&gt;_cl#_it#.per-base.bed.gz.csi</code>: Indexed bed file.</li> <li><code>&lt;sample-id&gt;_cl#_it#.mosdepth.summary.txt</code>: Summary metrics including mean, min and max coverage values.</li> <li><code>&lt;sample-id&gt;_cl#_it#.mosdepth.global.dist.txt</code>: A cumulative distribution indicating the proportion of total bases that were covered for at least a given coverage value.</li> </ul> <p>:::</p>"},{"location":"output/#variant-calling-filtering","title":"Variant calling &amp; filtering","text":"<p>Variant calling is done with <code>BCFTools mpileup</code> or <code>iVar</code>, the filtering with <code>BCFtools filter</code>.</p> <p>Variant files are visualized in the MultiQC report.</p> <p></p> <p>:::abstract{title=\"Output files - variants\" collapse}</p> <ul> <li><code>variants/variant_calling</code></li> <li><code>bcftools/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.vcf.gz</code>: A VCF file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.norm.vcf.gz</code>: A compressed VCF file where multiallelic sites are split up into biallelic records and SNPs and indels should be merged into a single record.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.filtered.vcf.gz</code>: A compressed VCF file containing the filtered variants.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.vcf.gz.tbi</code>: An index file for the compressed VCF file.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT_stats.txt</code>: A text file stats which is suitable for machine processing and can be plotted using plot-vcfstats.</li> </ul> </li> <li><code>ivar/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.ivar.tsv</code>: A tabular file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.ivar.vcf</code>: A VCF file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.ivar.variant_counts.log</code>: A summary file containing the number of indels and SNPs.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.filtered.vcf.gz</code>: A compressed VCF file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT_stats.txt</code>: A text file stats which is suitable for machine processing and can be plotted using plot-vcfstats.</li> </ul> </li> </ul> <p>:::</p> <p>:::abstract{title=\"Output files - iterations\" collapse}</p> <ul> <li><code>assembly/polishing/iterations/it#/variants/variant_calling</code></li> <li><code>bcftools/</code><ul> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.vcf.gz</code>: A VCF file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.norm.vcf.gz</code>: A compressed VCF file where multiallelic sites are split up into biallelic records and SNPs and indels should be merged into a single record.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.filtered.vcf.gz</code>: A compressed VCF file containing the filtered variants.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.vcf.gz.tbi</code>: An index file for the compressed VCF file.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.stats.txt</code>: A text file stats which is suitable for machine processing and can be plotted using plot-vcfstats.</li> </ul> </li> <li><code>ivar/</code><ul> <li><code>&lt;sample-id&gt;_cl#_it#.ivar.tsv</code>: A tabular file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.ivar.vcf</code>: A VCF file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.ivar.variant_counts.log</code>: A summary file containing the number of indels and SNPs.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.filtered.vcf.gz</code>: A compressed VCF file containing the variant calls.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_cl#_it#.stats.txt</code>: A text file stats which is suitable for machine processing and can be plotted using plot-vcfstats.</li> </ul> </li> </ul> <p>:::</p>"},{"location":"output/#variant-annotation","title":"Variant annotation","text":"<p>Variant annotation is performed with <code>SnpEff</code> to predict the functional effects of identified variants, and <code>SnpSift</code> to extract information from the annotated variants into a tabular format.</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>variants/annotation/</code></li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.ann.vcf.gz</code>: A VCF file with annotated variants showing their predicted functional effects.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.csv</code>: A CSV file with statistics about the variant annotation.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.genes.txt</code>: A text file with gene-level statistics.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.summary.html</code>: An HTML summary of the variant annotation statistics.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;constraint-id&gt;-CONSTRAINT.snipsift.table.txt</code>: A tabular file containing extracted fields from the annotated variants, making the information more accessible for analysis.</li> </ul> <p>:::</p> <p>The SnpEff annotation provides valuable information about variants, including:</p> <ul> <li>Variant type (SNP, insertion, deletion)</li> <li>Impact (HIGH, MODERATE, LOW, MODIFIER)</li> <li>Gene name and ID affected by the variant</li> <li>Type of effect (missense, synonymous, frameshift, etc.)</li> <li>Changes at the amino acid level (protein impact)</li> <li>Location within gene features (exon, intron, etc.)</li> </ul> <p>By default, variant annotation is enabled. It can be skipped with <code>--skip_vcf_annotation</code>.</p>"},{"location":"output/#consensus-generation","title":"Consensus generation","text":"<p>The consensus sequences are generated by <code>BCFTools</code> or <code>iVar</code>. The consensus sequences are stored in the directory <code>consensus/</code> or in the iterations directory <code>assembly/polishing/iterations/it#/consensus</code>.</p> <p><code>BCFtools</code> will use the filtered variants file whereas, <code>iVar</code> will redetermine the variants to collapse in the consensus using their own workflow, read more about their differences in the consensus calling section.</p> <p>:::abstract{title=\"Output files - iterations &amp; variants\" collapse}</p> <ul> <li><code>consensus</code></li> <li><code>seq/&lt;it# | scaffold_consensus | variant-calling | constraint&gt;/</code><ul> <li><code>&lt;sample-id&gt;/*.fasta</code>: A fasta file containing the consensus sequence.</li> </ul> </li> <li><code>mask/&lt;it# | variant-calling | constraint&gt;</code><ul> <li><code>&lt;sample-id&gt;/*.qual.txt</code>: A log file of the consensus run containing statistics. [<code>iVar</code> only]</li> <li><code>&lt;sample-id&gt;/*.bed</code>: A bed file containing the masked regions. [<code>BCFtools</code> only]</li> <li><code>&lt;sample-id&gt;/*.mpileup</code>: A mpileup file containing information on the depth and the quality of each aligned base.</li> </ul> </li> </ul> <p>:::</p>"},{"location":"output/#consensus-quality-control","title":"Consensus Quality control","text":"<p>Consensus quality control is done with multiple tools, the results are stored in the directory <code>consensus/quality_control/</code>.</p>"},{"location":"output/#quast","title":"Quast","text":"<p>QUAST is a quality assessment tool for genome assemblies. It calculates various metrics such as N50, L50, number of contigs, number of mismatches, number of indels, etc.</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>consensus/quality_control/quast/</code></li> <li><code>&lt;sample-id&gt;/&lt;iteration&gt;/&lt;sample-id&gt;_&lt;cl# | constraint-id&gt;.tsv</code>: A tabular file containing the QUAST report.</li> </ul> <p>:::</p> <p>If no iterative refinement was run, the output will be in the <code>consensus/quality_control/quast/&lt;sample-id&gt;/constraint</code> directory.</p>"},{"location":"output/#checkv","title":"CheckV","text":"<p><code>CheckV</code> is a tool for assessing the quality of viral genomes recovered from metagenomes. It calculates various metrics such as the number of viral genes, the number of viral contigs, the number of viral genomes, etc.</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>consensus/quality_control/checkv/</code></li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;cl# | constraint-id&gt;/quality_summary.tsv</code>: A tabular file that integrates the results from the three main modules of checkv and should be the main output referred to.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;cl# | constraint-id&gt;/completeness.tsv</code>: A detailed overview of how completeness was estimated.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;cl# | constraint-id&gt;/contamination.tsv</code>: A detailed overview of how contamination was estimated.</li> <li><code>&lt;sample-id&gt;/&lt;sample-id&gt;_&lt;cl# | constraint-id&gt;/complete_genomes.tsv</code>: A detailed overview of putative genomes identified.</li> </ul> <p>:::</p>"},{"location":"output/#prokka","title":"Prokka","text":"<p><code>Prokka</code> is a whole genome annotation pipeline for identifying features of interest in a set of genomic DNA sequences, and labelling them with useful information. Prokka is a software tool to annotate bacterial, archaeal and viral genomes.</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>consensus/quality_control/prokka/</code></li> <li>`//* directories containing the prokka output files. <p>:::</p>"},{"location":"output/#blastn","title":"BLASTn","text":"<p>BLAST is a tool for comparing primary biological sequence information. The output from the BLAST run is stored in the directory <code>consensus/quality_control/blast/</code>. Final consensus genomes are searched against the <code>--reference_pool</code>.</p> <p>:::info{title=\"Column names\" collapse}</p> <ul> <li>qseqid</li> <li>sseqid</li> <li>stitle</li> <li>pident</li> <li>qlen</li> <li>slen</li> <li>length</li> <li>mismatch</li> <li>gapopen</li> <li>qstart</li> <li>qend</li> <li>sstart</li> <li>send</li> <li>evalue</li> <li>bitscore</li> </ul> <p>:::</p> <p>::: tip{title= \"Modifying blast columns\"}</p> <p>Modifying these columns can be done through a custom config file and by updating <code>bin/utils/constant_variables.py</code>.</p> <p>:::</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>consensus/quality_control/blast/</code> A tabular file containing the BLAST report of all intermediate &amp; final results.</li> </ul> <p>:::</p>"},{"location":"output/#mmseqs-search-annotation","title":"MMseqs-search (annotation)","text":"<p>MMseqs-search is an ultra-fast and sensitive search tool for protein and nucleotide databases. Viralmetagenome uses MMseqs to search the consensus genomes in an annotated database, like Virousarus (see also defining your own custom annotation database), and uses the annotation data of the best hit to assign the consensus genome a species name, segment name, expected host, and any other metadata that is embedded within the database.</p> <p>:::info{title=\"Column names\" collapse}</p> <ul> <li>qseqid</li> <li>sseqid</li> <li>stitle</li> <li>pident</li> <li>qlen</li> <li>slen</li> <li>length</li> <li>mismatch</li> <li>gapopen</li> <li>qstart</li> <li>qend</li> <li>sstart</li> <li>send</li> <li>evalue</li> <li>bitscore</li> </ul> <p>:::</p> <p>:::tip{title=\"Modifying mmseqs columns\"}</p> <p>Modifying these columns can be done through a custom config file and by updating <code>bin/utils/constant_variables.py</code>.</p> <p>:::</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>consensus/quality_control/mmseqs-search/all_genomes_annotation.hits.tsv</code>: A tabular file containing the MMseqs-search hits, all genomes are combined to reduce the number of jobs.</li> </ul> <p>:::</p>"},{"location":"output/#mafft","title":"MAFFT","text":"<p>MAFFT is a multiple sequence alignment program for amino acid or nucleotide sequences. The output from the MAFFT run is stored in the directory <code>consensus/quality_control/mafft/</code>.</p> <p>It is used to align the following genomic data:</p> <ul> <li>The final consensus genome</li> <li>The identified reference genome from <code>--reference_pool</code></li> <li>The denovo contigs from each assembler (that constituted the final consensus genome)</li> <li>Each consensus genome from the iterative refinement steps.</li> </ul> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>consensus/quality_control/mafft/</code></li> <li><code>&lt;sample-id&gt;/&lt;sample_id&gt;_cl#_iterations.fas</code>: A fasta file containing a multiple sequence alignment of only the iterations.</li> <li><code>&lt;sample-id&gt;/&lt;sample_id&gt;_cl#_aligned.fas</code>: A fasta file containing a multiple sequence alignment of the denovo contigs, the reference from reference_pool and the consensus from iterations.</li> </ul> <p>:::</p> <p>Alignment can then be opened with MSA viewer, for example Jalview</p> <p></p>"},{"location":"output/#multiqc","title":"MultiQC","text":"<p>MultiQC is a visualization tool that generates a single HTML report summarizing all samples in your project. Most of the pipeline QC results are visualized in the report and further statistics are available in the report data directory.</p> <p>Results generated by MultiQC collate pipeline QC from supported tools e.g. FastQC. The pipeline has special steps which also allow the software versions to be reported in the MultiQC output for future traceability. For more information about how to use MultiQC reports, see http://multiqc.info.</p> <p>Furthermore, viralmetagenome runs MultiQC 2 times, as it uses the output from multiqc to create multiple summary tables of the consensus genomes and their iterations.</p> <p>:::abstract{title=\"Output files Multiqc\" collapse}</p> <ul> <li><code>multiqc/</code></li> <li><code>multiqc_report.html</code>: a standalone HTML file that can be viewed in your web browser.</li> <li><code>multiqc_data/</code>: directory containing parsed statistics from the different tools used in the pipeline.</li> <li><code>multiqc_plots/</code>: directory containing static images from the report in various formats.</li> </ul> <p>:::</p> <p>:::abstract{title=\"Custom overview tables\" collapse}</p> <ul> <li><code>overview-tables/</code>: a directory with a set of summary TSV files.</li> <li><code>contigs_overview_with_iterations.tsv</code>: A tabular file containing the contig information of the final contig consensus genome and their intermediate iterations.</li> <li><code>contigs_overview.tsv</code>: A tabular file containing the contig information of the final contig consensus genome.</li> <li><code>mapping_overview.tsv</code>: A tabular file containing the mapping information of the final mapped consensus genome, from the argument <code>--mapping_constraints</code>.</li> <li><code>samples_overview.tsv</code>: A tabular file containing the sample information combining information from both <code>contigs_overview.tsv</code> &amp; <code>mapping_overview.tsv</code>.</li> </ul> <p>:::</p> <p>:::abstract{title=\"Output files\" collapse}</p> <ul> <li><code>pipeline_info/</code></li> <li>Reports generated by Nextflow: <code>execution_report.html</code>, <code>execution_timeline.html</code>, <code>execution_trace.txt</code> and <code>pipeline_dag.dot</code>/<code>pipeline_dag.svg</code>.</li> <li>Reports generated by the pipeline: <code>pipeline_report.html</code>, <code>pipeline_report.txt</code> and <code>software_versions.yml</code>. The <code>pipeline_report*</code> files will only be present if the <code>--email</code> / <code>--email_on_fail</code> parameter's are used when running the pipeline.</li> <li>Reformatted samplesheet files used as input to the pipeline: <code>samplesheet.valid.csv</code>.</li> <li>Parameters used by the pipeline run: <code>params.json</code>.</li> </ul> <p>:::</p> <p>Nextflow provides excellent functionality for generating various reports relevant to the running and execution of the pipeline. This will allow you to troubleshoot errors with the running of the pipeline, and also provide you with other information such as launch commands, run times and resource usage.</p>"},{"location":"parameters/","title":"nf-core/viralmetagenome pipeline parameters","text":"<p>a bioinformatics best-practice analysis pipeline for untargeted viral genome reconstruction and to identify intra-host variants from metagenomic sequencing data.</p>"},{"location":"parameters/#inputoutput-options","title":"Input/output options","text":"<p>Define where the pipeline should find input data and save output data.</p> Parameter Description Default <code>input</code> Path to comma-separated file containing information about the samples in the experiment. HelpYou will need to create a design file with information about the samples in your experiment before running the pipeline. Use this parameter to specify its location. It has to be a comma-separated file with 3 columns, and a header row. See usage docs. <code>outdir</code> The output directory where the results will be saved. You have to use absolute paths to storage on Cloud infrastructure. <code>metadata</code> Sample metadata that is included in the multiqc report <code>email</code> Email address for completion summary. HelpSet this parameter to your e-mail address to get a summary e-mail with details of the run sent to you when the workflow exits. If set in your user config file (<code>~/.nextflow/config</code>) then you don't need to specify this on the command line for every run."},{"location":"parameters/#preprocessing-options","title":"Preprocessing options","text":"<p>Options related to the trimming, low complexity and host removal steps of the reads</p> Parameter Description Default <code>skip_preprocessing</code> Skip read preprocessing and use input reads for downstream analysis <code>skip_fastqc</code> Skip read quality statistics summary tool 'fastqc' <code>save_final_reads</code> Save reads after the final preprocessing step True <code>save_intermediate_reads</code> Save reads after every preprocessing step <code>with_umi</code> With or without UMI detection <code>skip_umi_extract</code> With or without UMI extraction True <code>umi_deduplicate</code> Specify at what level UMI deduplication should occur. read <code>umi_discard_read</code> Discard R1 / R2 if required 0, meaning not to discard 0 <code>trim_tool</code> The used trimming tool fastp <code>skip_trimming</code> Skip read trimming <code>adapter_fasta</code> Fasta file of adapters <code>save_trimmed_fail</code> Specify true to save files that failed to pass trimming thresholds ending in <code>*.fail.fastq.gz</code> <code>save_merged</code> Specify true to save all merged reads to a file ending in <code>*.merged.fastq.gz</code> <code>min_trimmed_reads</code> Inputs with fewer than this reads will be filtered out of the \"reads\" output channel 1 <code>merge_reads</code> Specify if reads coming from the same group or orignal sample should be merged for the downstream analyses <code>skip_complexity_filtering</code> Skip filtering of low complexity regions in reads HelpLow-complexity sequences are defined as having commonly found stretches of nucleotides with limited information content (e.g. the dinucleotide repeat CACACACACA). Such sequences can produce a large number of high-scoring but biologically insignificant results in database searches True <code>decomplexifier</code> Specify the decomplexifier to use, bbduk or prinseq prinseq <code>contaminants</code> Reference files containing adapter and/or contaminant sequences for sequence kmer matching (used by bbduk) <code>skip_hostremoval</code> Skip the removal of host read sequences <code>host_k2_db</code> Kraken2 database used to remove host and contamination s3://ngi-igenomes/test-data/viralrecon/kraken2_human.tar.gz <code>skip_host_fastqc</code> Skip the fastqc step after host &amp; contaminants were removed <code>arguments_fastqc</code> Arguments for FastQC tool --quiet <code>arguments_fastp</code> Arguments for Fastp tool --cut_front --cut_tail --trim_poly_x --cut_mean_quality 30 --qualified_quality_phred 30 --unqualified_percent_limit 10 --length_required 50 <code>arguments_trimmomatic</code> Arguments for Trimmomatic tool ILLUMINACLIP:null:2:30:10 <code>arguments_umitools_extract</code> Arguments for UMI-tools extract --umi-separator \": <code>arguments_humid</code> Arguments for Humid tool -a -m 1 <code>arguments_bbduk</code> Arguments for BBDuk tool entropy=0.3 entropywindow=50 entropymask=f <code>arguments_prinseq_reads</code> Arguments for Prinseq tool for reads <code>arguments_kraken2_host</code> Arguments for Kraken2 tool for host removal"},{"location":"parameters/#metagenomic-diversity","title":"Metagenomic diversity","text":"<p>Parameters used to determine the metagenomic diversity of the sample</p> Parameter Description Default <code>skip_read_classification</code> Skip determining the metagenomic diversity of the sample <code>read_classifiers</code> Specify the taxonomic read classifiers, choices are 'kaiju,kraken2' kraken2,kaiju <code>save_databases</code> Save the used databases <code>kraken2_db</code> Location of the Kraken2 database https://genome-idx.s3.amazonaws.com/kraken/k2_viral_20230314.tar.gz <code>kraken2_save_reads</code> Save classified and unclassified reads as fastq files <code>kraken2_save_readclassification</code> Save summary overview of read classifications in a txt file <code>kraken2_save_minimizers</code> Save kraken2's used minimizers <code>bracken_db</code> Location of bracken database https://genome-idx.s3.amazonaws.com/kraken/k2_viral_20230314.tar.gz <code>kaiju_db</code> Location of Kaiju database https://kaiju-idx.s3.eu-central-1.amazonaws.com/2023/kaiju_db_rvdb_2023-05-26.tgz <code>kaiju_taxon_rank</code> Level of taxa rank that needs to be determined species <code>arguments_kraken2</code> Arguments for Kraken2 tool --report-minimizer-data <code>arguments_kaiju</code> Arguments for Kaiju tool -v <code>arguments_kaiju2table</code> Arguments for Kaiju2Table tool -e -l species <code>arguments_kaiju2krona</code> Arguments for Kaiju2Krona tool -v -u <code>arguments_krona</code> Arguments for Krona tool <code>arguments_bracken</code> Arguments for Bracken tool <code>arguments_kreport2krona</code> Arguments for Kreport2Krona tool"},{"location":"parameters/#assembly","title":"Assembly","text":"<p>Parameters relating to the used assembly methods</p> Parameter Description Default <code>skip_assembly</code> Skip de novo assembly of reads <code>assemblers</code> The specified tools for de novo assembly, multiple options are possible spades,megahit <code>spades_mode</code> Specific SPAdes mode to run rnaviral <code>spades_hmm</code> File or directory with amino acid HMMs for Spades HMM-guided mode. <code>spades_yml</code> Path to yml file containing read information. HelpThe raw FASTQ files listed in this YAML file MUST be supplied to the respective illumina/pacbio/nanopore input channel(s) in addition to this YML. File entries in this yml must contain only the file name and no paths. <code>skip_contig_prinseq</code> Skip the filtering of low complexity contigs with prinseq <code>skip_sspace_basic</code> Skip the contig extension with sspace_basic True <code>read_distance</code> Specify the mean distance between the paired reads 350 <code>read_distance_sd</code> Specify the deviation of the mean distance that is allowed. HelpFor instance, a mean of 200 and a sd of 0.75. This means that any pair having a distance between 150 and 250 is allowed. 0.75 <code>read_orientation</code> Specify the read orientation. FR <code>arguments_spades</code> Arguments for SPAdes tool --rnaviral <code>arguments_megahit</code> Arguments for MEGAHIT tool <code>arguments_trinity</code> Arguments for Trinity tool --max_reads_per_graph 100000 <code>arguments_quast</code> Arguments for QUAST tool --min-contig 0 <code>arguments_sspace_basic</code> Arguments for SSPACE Basic tool -x 1 -o 15 -r 0.75 <code>arguments_prinseq_contig</code> Arguments for Prinseq tool for contigs -out_format 1 -lc_dust .20"},{"location":"parameters/#polishing","title":"Polishing","text":"<p>Parameters relating to the refinement of de novo contigs</p> Parameter Description Default <code>skip_polishing</code> Skip the refinement/polishing of contigs through reference based scaffolding and read mapping <code>save_intermediate_polishing</code> Save intermediate polishing files HelpThere are multiple processes within the polishing subworkflow that might not contain relevant information <code>reference_pool</code> Set of fasta sequences used as potential references for the contigs https://rvdb.dbi.udel.edu/download/C-RVDBvCurrent.fasta.gz <code>skip_precluster</code> Skip the preclustering of assemblies to facilitate downstream processing of assemblies <code>keep_unclassified</code> Keep the contigs that could not be classified with the taxonomic databases (<code>kaiju_db</code> &amp; <code>kraken2_db</code>) HelpWithin the preclustering step, all contigs will get a taxonomic classification using the provided databases for the metagenomic tools. In some cases, the number of unclassified contigs can be very large if the database is restrictive. This will result in large clusters in downstream processing that can take up a lot of resources despite not being a priority in some analyses. So set it to <code>True</code> if you want to keep unclassified contigs and set it to <code>False</code> if you don't want to keep them.  True <code>precluster_classifiers</code> Specify the metagenomic classifiers to use for contig taxonomy classification: 'kraken2,kaiju' kraken2,kaiju <code>cluster_method</code> Cluster algorithm used for contigs cdhitest <code>network_clustering</code> (only with mash) Algorithm to partition the network. HelpMash creates a distance matrix that gets translated into a network of connectected nodes where the edges represent the similarity. This network is then split up using the specified method. - leiden algorithm: a hierarchical clustering algorithm, that recursively merges communities into single nodes by greedily optimizing the modularity - [connected_components] algorithm: a clustering algorithm that defines the largest possible communities where each node within a subset is reachable from every other node in the same subset via any edge . connected_components <code>skip_nocov_to_reference</code> Skip creation of the hybrid consensus, instead keep the scaffold with ambiguous bases if the depth of scaffolds is not high enough. <code>identity_threshold</code> Identity threshold value used in clustering algorithms 0.85 <code>perc_reads_contig</code> Minimum cumulated sum of mapped read percentages of each member from a cluster group, set to 0 to disable HelpSetting this variable will remove clusters that have a low cumulated sum of mapped read percentages. This can be used to remove clusters that have a low coverage and are likely to be false positives. 5 <code>min_contig_size</code> Minimum allowed contig size HelpSetting this to a low value will result in a large number of questionable contigs and an increase in computation time  500 <code>max_contig_size</code> Maximum allowed contig size 10000000 <code>max_n_perc</code> Define the maximum percentage of ambiguous bases in a contig 50 <code>skip_singleton_filtering</code> Skip the filtering of contigs that did not cluster together with other contigs HelpSetting this to true will cause the pipeline not to remove contigs that don't have similar contigs. Filtering settings can be further specified with <code>min_contig_size</code> and <code>max_n_100kbp</code>. <code>arguments_blast_makeblastdb</code> Arguments for BLAST makeblastdb tool -dbtype nucl <code>arguments_blastn</code> Arguments for BLASTN tool -max_target_seqs 5 <code>arguments_blast_filter</code> Arguments for BLAST filter tool --escore 0.01 --bitscore 50 --percent-alignment 0.80 <code>arguments_kraken2_contig</code> Arguments for Kraken2 tool for contigs <code>arguments_kaiju_contig</code> Arguments for Kaiju tool for contigs -v <code>arguments_extract_precluster</code> Arguments for precluster extraction --keep-unclassified true --merge-strategy lca <code>arguments_cdhit</code> Arguments for CD-HIT tool -c 0.85 -mask rRyYkKsSwWmMbBdDhHvVnN <code>arguments_vsearch</code> Arguments for VSEARCH tool --maxseqlength 10000000 --id 0.85 --strand both --iddef 0 --no_progress --qmask none <code>arguments_mmseqs_linclust</code> Arguments for MMseqs2 linclust tool --min-seq-id 0.85 -c 0.700 --cov-mode 2 --cluster-mode 0 <code>arguments_mmseqs_cluster</code> Arguments for MMseqs2 cluster tool --min-seq-id 0.85 -c 0.700 --cov-mode 2 --cluster-mode 0 <code>arguments_vrhyme</code> Arguments for VRhyme tool --mems 50 <code>arguments_mash_dist</code> Arguments for Mash distance tool -s 4000 -k 15 <code>arguments_network_cluster</code> Arguments for network clustering --score 0.85 <code>arguments_extract_cluster</code> Arguments for cluster extraction --perc_reads_contig 5 <code>arguments_minimap2_align</code> Arguments for Minimap2 alignment <code>arguments_minimap2_index</code> Arguments for Minimap2 index <code>arguments_mash_sketch</code> Arguments for Mash sketch tool -i <code>arguments_mash_screen</code> Arguments for Mash screen tool <code>arguments_select_reference</code> Arguments for selecting reference"},{"location":"parameters/#iterative-consensus-refinement","title":"Iterative consensus refinement","text":"<p>Define parameters for iterations to update de novo consensus using  reference based improvements</p> Parameter Description Default <code>skip_iterative_refinement</code> Don't realign reads to consensus sequences and redefine the consensus through (multiple) iterations <code>iterative_refinement_cycles</code> Number of iterations 2 <code>intermediate_mapper</code> Mapping tool used during iterations bwamem2 <code>intermediate_variant_caller</code> Variant caller used during iterations ivar <code>call_intermediate_variants</code> Call variants during the iterations HelpWill always be done when iterative consensus caller is bcftools <code>intermediate_consensus_caller</code> Consensus tool used for calling new consensus during iterations bcftools <code>intermediate_mapping_stats</code> Calculate summary statistics during iterations True"},{"location":"parameters/#variant-analysis","title":"Variant analysis","text":"<p>Parameters relating to the analysis of variants associated to contigs and scaffolds</p> Parameter Description Default <code>skip_variant_calling</code> Skip the analysis of variants for the external reference or contigs <code>skip_vcf_annotation</code> Skip the annotation of the VCF file <code>mapper</code> Define which mapping tool needs to be used when mapping reads to reference bwamem2 <code>mapping_constraints</code> Sequence to use as a mapping reference instead of the de novo contigs or scaffolds <code>deduplicate</code> Deduplicate the reads HelpIf used with UMI's, <code>umi tools</code> will be used to group and call consensus of each individual read group. If not used with UMI's use <code>PicardsMarkDuplicates</code>.  True <code>variant_caller</code> Define the variant caller to use: 'ivar' or 'bcftools' ivar <code>consensus_caller</code> Consensus tool used for calling new consensus in final iteration ivar <code>min_mapped_reads</code> Define the minimum number of mapped reads in order to continue the variant and consensus calling 200 <code>min_consensus_depth</code> Define the minimum consensus depth 5 <code>allele_frequency</code> Minimum allele frequency threshold for calling consensus 0.75 <code>mapping_stats</code> Calculate summary statistics in final iteration True <code>ivar_header</code> <code>arguments_bwamem2_index</code> Arguments for BWA-MEM2 index <code>arguments_bwa_index</code> Arguments for BWA index <code>arguments_bwa_mem</code> Arguments for BWA MEM <code>arguments_bowtie2_build</code> Arguments for Bowtie2 build <code>arguments_bowtie2_align</code> Arguments for Bowtie2 alignment --local --very-sensitive-local --seed 1 <code>arguments_umitools_dedup</code> Arguments for UMI-tools deduplication --umi-separator=\\':\\' --method cluster --unmapped-reads use <code>arguments_picard_markduplicates</code> Arguments for Picard MarkDuplicates --ASSUME_SORTED true --VALIDATION_STRINGENCY LENIENT --TMP_DIR tmp --REMOVE_DUPLICATES true <code>arguments_picard_collectmultiplemetrics</code> Arguments for Picard CollectMultipleMetrics --ASSUME_SORTED true --VALIDATION_STRINGENCY LENIENT --TMP_DIR tmp <code>arguments_custom_mpileup</code> Arguments for custom mpileup --max-depth 800000 <code>arguments_mosdepth</code> Arguments for Mosdepth tool <code>arguments_bcftools_mpileup1</code> Arguments for BCFtools mpileup step 1 --ignore-overlaps --count-orphans --max-depth 800000 --min-BQ 20 --annotate FORMAT/AD,FORMAT/ADF,FORMAT/ADR,FORMAT/DP,FORMAT/SP,INFO/AD,INFO/ADF,INFO/ADR <code>arguments_bcftools_mpileup2</code> Arguments for BCFtools mpileup step 2 --ploidy 2 --keep-alts --keep-masked-ref --multiallelic-caller --variants-only <code>arguments_bcftools_mpileup3</code> Arguments for BCFtools mpileup step 3 --include 'INFO/DP&gt;=5 <code>arguments_bcftools_norm</code> Arguments for BCFtools norm --do-not-normalize --output-type z --multiallelics -any <code>arguments_bcftools_stats</code> Arguments for BCFtools stats <code>arguments_samtools_stats</code> Arguments for Samtools stats command <code>arguments_samtools_idxstats</code> Arguments for Samtools idxstats command <code>arguments_samtools_flagstat</code> Arguments for Samtools flagstat command <code>arguments_tabix</code> Arguments for Tabix tool -p vcf -f <code>arguments_bedtools_merge</code> Arguments for Bedtools merge <code>arguments_bedtools_maskfasta</code> Arguments for Bedtools maskfasta <code>arguments_bcftools_consensus</code> Arguments for BCFtools consensus <code>arguments_ivar_variants1</code> Arguments for iVar variants step 1 -q 20 -m 5 <code>arguments_ivar_variants2</code> Arguments for iVar variants step 2 --ignore-overlaps --count-orphans --max-depth 0 --no-BAQ --min-BQ 0 <code>arguments_make_bed_mask</code> Arguments for making BED mask -a --ignore-overlaps --count-orphans --max-depth 0 --no-BAQ --min-BQ 0 <code>arguments_ivar_consensus1</code> Arguments for iVar consensus step 1 -t 0 -q 20 -m 5 -n N <code>arguments_ivar_consensus2</code> Arguments for iVar consensus step 2 --count-orphans --max-depth 0 --min-BQ 20 --no-BAQ -aa <code>arguments_snpeff</code> Arguments for SnpEff tool for variant annotation <code>arguments_snpsift_extractfields</code> Arguments for SnpSift ExtractFields tool -s \",\" -e \"."},{"location":"parameters/#consensus-qc","title":"Consensus QC","text":"<p>Apply different quality control techniques on the generated consensus genomes</p> Parameter Description Default <code>skip_consensus_qc</code> Skip the quality measurements on consensus genomes <code>skip_checkv</code> Skip the use of checkv for quality check <code>checkv_db</code> Reference database used by checkv for consensus quality control HelpIf not given, the most recent one is downloaded. <code>skip_consensus_annotation</code> Skip the annotation of the consensus constructs <code>annotation_db</code> Database used for annotation of the consensus constructs HelpThe metadata fields are stored in the fasta comment as <code>key1:\"value1\"|key2:\"value2\"|...</code> see docs/databases.md for more information. ftp://ftp.expasy.org/databases/viralzone/2020_4/virosaurus90_vertebrate-20200330.fas.gz <code>skip_prokka</code> Skip gene estimation &amp; annotation with prokka <code>prokka_db</code> Define a prokka <code>--protein</code> database for protein annotation HelpSpecify a custom protein database for Prokka annotation <code>skip_quast</code> Skip the use of QUAST for quality check <code>skip_blast_qc</code> Skip the blast search of contigs to the provided reference DB <code>skip_alignment_qc</code> Skip creating an alignment of each the collapsed clusters and each iterative step True <code>mmseqs_searchtype</code> Specify the search algorithm to use for mmseqs. 0: auto 1: amino acid, 2: translated, 3: nucleotide, 4: translated nucleotide alignment HelpOnly search-type 3 supports both forward and reverse search1 - BLASTP;2 - TBLASTN;3 - BLASTN;4 - TBLASTX 4 <code>arguments_checkv</code> Arguments for CheckV tool --remove_tmp <code>arguments_mafft_iterations</code> Arguments for MAFFT iterations --auto --adjustdirection <code>arguments_mafft_qc</code> Arguments for MAFFT QC --auto --adjustdirection <code>arguments_blastn_qc</code> Arguments for BLASTN QC -max_target_seqs 5 <code>arguments_prokka</code> Arguments for Prokka tool --centre X --compliant --force --kingdom Viruses <code>arguments_mmseqs_search</code> Arguments for MMseqs2 search --search-type 4 --rescore-mode 3 <code>arguments_quast_qc</code> Arguments for QUAST quality control"},{"location":"parameters/#institutional-config-options","title":"Institutional config options","text":"<p>Parameters used to describe centralised config profiles. These should not be edited.</p> Parameter Description Default <code>custom_config_version</code> Git commit id for Institutional configs. master <code>custom_config_base</code> Base directory for Institutional configs. HelpIf you're running offline, Nextflow will not be able to fetch the institutional config files from the internet. If you don't need them, then this is not a problem. If you do need them, you should download the files from the repo and tell Nextflow where to find them with this parameter. https://raw.githubusercontent.com/nf-core/configs/master <code>config_profile_name</code> Institutional config name. <code>config_profile_description</code> Institutional config description. <code>config_profile_contact</code> Institutional config contact information. <code>config_profile_url</code> Institutional config URL link."},{"location":"parameters/#generic-options","title":"Generic options","text":"<p>Less common options for the pipeline, typically set in a config file.</p> Parameter Description Default <code>version</code> Display version and exit. <code>publish_dir_mode</code> Method used to save pipeline results to output directory. HelpThe Nextflow <code>publishDir</code> option specifies which intermediate files should be saved to the output directory. This option tells the pipeline what method should be used to move these files. See Nextflow docs for details. copy <code>email_on_fail</code> Email address for completion summary, only when pipeline fails. HelpAn email address to send a summary email to when the pipeline is completed - ONLY sent if the pipeline does not exit successfully. <code>plaintext_email</code> Send plain-text email instead of HTML. <code>max_multiqc_email_size</code> File size limit when attaching MultiQC reports to summary emails. 25.MB <code>monochrome_logs</code> Do not use coloured log outputs. <code>hook_url</code> Incoming hook URL for messaging service HelpIncoming hook URL for messaging service. Currently, MS Teams and Slack are supported. <code>multiqc_title</code> MultiQC report title. Printed as page header, used for filename if not otherwise specified. <code>multiqc_config</code> Custom config file to supply to MultiQC. <code>multiqc_logo</code> Custom logo file to supply to MultiQC. File name must also be set in the MultiQC config file <code>multiqc_methods_description</code> Custom MultiQC yaml file containing HTML including a methods description. <code>custom_table_headers</code> Custom yaml file containing the table column names selection and new names. <code>validate_params</code> Boolean whether to validate parameters against the schema at runtime True <code>pipelines_testdata_base_path</code> Base URL or local path to location of pipeline test dataset files https://raw.githubusercontent.com/nf-core/test-datasets/ <code>trace_report_suffix</code> Suffix to add to the trace report filename. Default is the date and time in the format yyyy-MM-dd_HH-mm-ss. <code>prefix</code> Prefix of all output files followed by [date][pipelineversion][runName] HelpUse '--global_prefix' to not have metadata embedded. <code>global_prefix</code> Global prefix set if you don't want metadata embedded in the output filenames"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#please-read-this-documentation-on-the-nf-core-website-httpsnf-coreviralmetagenomeusage","title":"Please read this documentation on the nf-core website: https://nf-co.re/viralmetagenome/usage","text":"<pre><code>nextflow run nf-core/viralmetagenome -profile test,docker\n</code></pre> <p>Make sure you have Nextflow and a container manager (for example, Docker) installed. See the installation instructions for more info.</p> <p>:::tip Did your analysis fail? After fixing the issue add <code>-resume</code> to the command to continue from where it left off. :::</p>"},{"location":"usage/#input","title":"Input","text":""},{"location":"usage/#samples","title":"Samples","text":"<p>The pipeline requires a samplesheet as input. This samplesheet should contain the name and the absolute locations of reads.</p> <pre><code>--input '[path to samplesheet file]'\n</code></pre> <p>The pipeline will auto-detect whether a sample is single- or paired-end using the information provided in the samplesheet (i.e. if the <code>fastq_2</code> column is empty, the sample is assumed to be single-end).</p> <p>An example samplesheet file consisting of both single- and paired-end data may look something like the one below.</p> input-samplesheet.csv<pre><code>sample,fastq_1,fastq_2\nsample1,AEG588A1_S1_L002_R1_001.fastq.gz,AEG588A1_S1_L002_R2_001.fastq.gz\nsample2,AEG588A5_S5_L003_R1_001.fastq.gz,\nsample3,AEG588A3_S3_L002_R1_001.fastq.gz,AEG588A3_S3_L002_R2_001.fastq.gz\n</code></pre> Value Description <code>sample</code> Custom sample name, needs to be unique <code>fastq_1</code> Full path (not relative paths) to FastQ file for Illumina short reads 1. File has to be gzipped and have the extension \".fastq.gz\" or \".fq.gz\". <code>fastq_2</code> Full path (not relative paths) to FastQ file for Illumina short reads 2. File has to be gzipped and have the extension \".fastq.gz\" or \".fq.gz\"."},{"location":"usage/#mapping-constraints","title":"Mapping constraints","text":"<p>Viralmetagenome can in addition to constructing de novo consensus genomes map the sample reads to a series of references. These references are provided through the parameter <code>--mapping_constraints</code>.</p> <p>An example mapping constraint samplesheet file consisting of 5 references, may look something like the one below.</p> <p>This is for 5 references, 2 of them being a multi-fasta file, only one of the multi-fasta needs to undergo reference selection.</p> constraints-samplesheet.tsv<pre><code>id species segment selection samples sequence definition\nLassa-L-dataset LASV L true LASV_L.multi.fasta Collection of LASV sequences used for hybrid capture bait design, all publicly available sequences of the L segment clustered at 99.5% similarity\nLassa-S-dataset LASV S false sample1;sample3 LASV_S.multi.fasta Collection of LASV sequences used for hybrid capture bait design, all publicly available sequences of the S segment clustered at 99.5% similarity\nNC038709.1 HAZV L false sample1;sample2 L-NC_038709.1.fasta Hazara virus isolate JC280 segment L, complete sequence.\nNC038710.1 HAZV M false M-NC_038710.1.fasta Hazara virus isolate JC280 segment M, complete sequence.\nNC038711.1 HAZV S false S-NC_038711.1.fasta Hazara virus isolate JC280 segment S, complete sequence.\n</code></pre> Column Description <code>id</code> Reference identifier, needs to be unique <code>species</code> [Optional] Species name of the reference <code>segment</code> [Optional] Segment name of the reference <code>selection</code> [Optional] Specify if the multi-fasta reference file needs to undergo reference selection <code>samples</code> [Optional] List of samples that need to be mapped towards the reference. If empty, map all samples. <code>sequence</code> Full path (not relative paths) to the reference sequence file. <code>gff</code> Full path (not relative paths) to the reference annotation gff file. <code>definition</code> [Optional] Definition of the reference sequence file. <p>:::tip</p> <ul> <li>The <code>samples</code> column is optional - if empty, all samples will be mapped towards the reference.</li> <li>Multi-fasta files can be provided and all reads will be mapped to all genomes but stats will not be reported separately in the final report.</li> </ul> <p>:::</p>"},{"location":"usage/#metadata","title":"Metadata","text":"<p>Sample metadata can be provided to the pipeline with the argument <code>--metadata</code>. This metadata will not affect the analysis in any way and is only used to annotate the final report. Any metadata can be provided as long as the first value is the <code>sample</code> value.</p> metadata.csv<pre><code>sample,sample_accession,secondary_sample_accession,study_accession,run_alias,library_layout\nsample1,SAMN14154201,SRS6189918,PRJNA607948,vero76_Illumina.fastq,PAIRED\nsample2,SAMN14154205,SRS6189924,PRJNA607948,veroSTAT-1KO_Illumina.fastq,PAIRED\n</code></pre>"},{"location":"usage/#running-the-pipeline","title":"Running the pipeline","text":"<p>The typical command for running the pipeline is as follows:</p> <pre><code>nextflow run nf-core/viralmetagenome --input ./samplesheet.csv --outdir ./results  -profile docker\n</code></pre> <p>This will launch the pipeline with the <code>docker</code> configuration profile. See below for more information about profiles.</p> <p>Note that the pipeline will create the following files in your working directory:</p> <pre><code>work          #(1)!\n&lt;OUTDIR&gt;      #(2)!\n.nextflow_log #(3)!\n...           #(4)!\n</code></pre> <p>If you wish to repeatedly use the same parameters for multiple runs, rather than specifying each flag in the command, you can specify these in a params file.</p> <p>Pipeline settings can be provided in a <code>yaml</code> or <code>json</code> file via <code>-params-file &lt;file&gt;</code>.</p> <p>[!WARNING] Do not use <code>-c &lt;file&gt;</code> to specify parameters as this will result in errors. Custom config files specified with <code>-c</code> must only be used for tuning process resource specifications, other infrastructural tweaks (such as output directories), or module arguments (args).</p> <p>The above pipeline run specified with a params file in yaml format:</p> <pre><code>nextflow run nf-core/viralmetagenome -profile docker -params-file params.yaml\n</code></pre> <p>with:</p> params.yaml<pre><code>input: './samplesheet.csv'\noutdir: './results/'\n&lt;...&gt;\n</code></pre> <p>You can also generate such <code>YAML</code>/<code>JSON</code> files via nf-core/launch.</p>"},{"location":"usage/#updating-the-pipeline","title":"Updating the pipeline","text":"<p>When you run the above command, Nextflow automatically pulls the pipeline code from GitHub and stores it as a cached version. When running the pipeline after this, it will always use the cached version if available - even if the pipeline has been updated since. To make sure that you're running the latest version of the pipeline, make sure that you regularly update the cached version of the pipeline:</p> <pre><code>nextflow pull nf-core/viralmetagenome\n</code></pre>"},{"location":"usage/#reproducibility","title":"Reproducibility","text":"<p>It is a good idea to specify the pipeline version when running the pipeline on your data. This ensures that a specific version of the pipeline code and software are used when you run your pipeline. If you keep using the same tag, you'll be running the same version of the pipeline, even if there have been changes to the code since.</p> <p>First, go to the nf-core/viralmetagenome releases page and find the latest pipeline version - numeric only (eg. <code>1.3.1</code>). Then specify this when running the pipeline with <code>-r</code> (one hyphen) - eg. <code>-r 1.3.1</code>. Of course, you can switch to another version by changing the number after the <code>-r</code> flag.</p> <p>This version number will be logged in reports when you run the pipeline, so that you'll know what you used when you look back in the future.</p> <p>To further assist in reproducibility, you can use share and reuse parameter files to repeat pipeline runs with the same settings without having to write out a command with every single parameter.</p> <p>[!TIP] If you wish to share such profile (such as upload as supplementary material for academic publications), make sure to NOT include cluster specific paths to files, nor institutional specific profiles.</p>"},{"location":"usage/#core-nextflow-arguments","title":"Core Nextflow arguments","text":"<p>[!NOTE] These options are part of Nextflow and use a single hyphen (pipeline parameters use a double-hyphen)</p>"},{"location":"usage/#the-profile-parameter","title":"The <code>-profile</code> parameter","text":"<p>Use this parameter to choose a configuration profile. Profiles can give configuration presets for different compute environments.</p> <p>Several generic profiles are bundled with the pipeline which instruct the pipeline to use software packaged using different methods (Docker, Singularity, Podman, Shifter, Charliecloud, Apptainer, Conda) - see below.</p> <p>[!IMPORTANT] We highly recommend the use of Docker or Singularity containers for full pipeline reproducibility, however when this is not possible, Conda is also supported.</p> <p>The pipeline also dynamically loads configurations from https://github.com/nf-core/configs when it runs, making multiple config profiles for various institutional clusters available at run time. For more information and to check if your system is supported, please see the nf-core/configs documentation.</p> <p>Note that multiple profiles can be loaded, for example: <code>-profile test,docker</code> - the order of arguments is important! They are loaded in sequence, so later profiles can overwrite earlier profiles.</p> <p>If <code>-profile</code> is not specified, the pipeline will run locally and expect all software to be installed and available on the <code>PATH</code>. This is not recommended, since it can lead to different results on different machines dependent on the computer environment.</p> <ul> <li><code>test</code></li> <li>A profile with a complete configuration for automated testing</li> <li>Includes links to test data so needs no other parameters</li> <li><code>docker</code></li> <li>A generic configuration profile to be used with Docker</li> <li><code>singularity</code></li> <li>A generic configuration profile to be used with Singularity</li> <li><code>podman</code></li> <li>A generic configuration profile to be used with Podman</li> <li><code>shifter</code></li> <li>A generic configuration profile to be used with Shifter</li> <li><code>charliecloud</code></li> <li>A generic configuration profile to be used with Charliecloud</li> <li><code>apptainer</code></li> <li>A generic configuration profile to be used with Apptainer</li> <li><code>wave</code></li> <li>A generic configuration profile to enable Wave containers. Use together with one of the above (requires Nextflow <code>24.03.0-edge</code> or later).</li> <li><code>conda</code></li> <li>A generic configuration profile to be used with Conda. Please only use Conda as a last resort i.e. when it's not possible to run the pipeline with Docker, Singularity, Podman, Shifter, Charliecloud, or Apptainer.</li> </ul>"},{"location":"usage/#-resume","title":"<code>-resume</code>","text":"<p>Specify this when restarting a pipeline. Nextflow will use cached results from any pipeline steps where the inputs are the same, continuing from where it got to previously. For input to be considered the same, not only the names must be identical but the files' contents as well. For more info about this parameter, see this blog post.</p> <p>You can also supply a run name to resume a specific run: <code>-resume [run-name]</code>. Use the <code>nextflow log</code> command to show previous run names.</p>"},{"location":"usage/#-c","title":"<code>-c</code>","text":"<p>Specify the path to a specific config file (this is a core Nextflow command). See the nf-core website documentation for more information.</p>"},{"location":"usage/#custom-configuration","title":"Custom configuration","text":""},{"location":"usage/#resource-requests","title":"Resource requests","text":"<p>Whilst the default requirements set within the pipeline will hopefully work for most people and with most input data, you may find that you want to customise the compute resources that the pipeline requests. Each step in the pipeline has a default set of requirements for number of CPUs, memory and time. For most of the pipeline steps, if the job exits with any of the error codes specified here it will automatically be resubmitted with higher resources request (2 x original, then 3 x original). If it still fails after the third attempt then the pipeline execution is stopped.</p> <p>To change the resource requests, please see the max resources and tuning workflow resources section of the nf-core website.</p>"},{"location":"usage/#custom-containers","title":"Custom Containers","text":"<p>In some cases, you may wish to change the container or conda environment used by a pipeline steps for a particular tool. By default, nf-core pipelines use containers and software from the biocontainers or bioconda projects. However, in some cases the pipeline specified version maybe out of date.</p> <p>To use a different container from the default container or conda environment specified in a pipeline, please see the updating tool versions section of the nf-core website.</p>"},{"location":"usage/#custom-tool-arguments","title":"Custom Tool Arguments","text":"<p>A pipeline might not always support every possible argument or option of a particular tool used in pipeline. Fortunately, nf-core pipelines provide some freedom to users to insert additional parameters that the pipeline does not include by default.</p> <p>To learn how to provide additional arguments to a particular tool of the pipeline, please see the customising tool arguments section of the nf-core website.</p>"},{"location":"usage/#nf-coreconfigs","title":"nf-core/configs","text":"<p>In most cases, you will only need to create a custom config as a one-off but if you and others within your organisation are likely to be running Viralmetagenome regularly and need to use the same settings regularly it may be a good idea to request that your custom config file is uploaded to the <code>nf-core/configs</code> git repository. Before you do this please can you test that the config file works with your pipeline of choice using the <code>-c</code> parameter. You can then create a pull request to the <code>nf-core/configs</code> repository with the addition of your config file, associated documentation file (see examples in <code>nf-core/configs/docs</code>), and amending <code>nfcore_custom.config</code> to include your custom profile.</p> <p>See the main Nextflow documentation for more information about creating your own configuration files.</p> <p>If you have any questions or issues please send us a message on Slack on the <code>#configs</code> channel.</p>"},{"location":"usage/#running-in-the-background","title":"Running in the background","text":"<p>Nextflow handles job submissions and supervises the running jobs. The Nextflow process must run until the pipeline is finished.</p> <p>The Nextflow <code>-bg</code> flag launches Nextflow in the background, detached from your terminal so that the workflow does not stop if you log out of your session. The logs are saved to a file.</p> <p>Alternatively, you can use <code>screen</code> / <code>tmux</code> or similar tool to create a detached session which you can log back into at a later time. Some HPC setups also allow you to run nextflow within a cluster job submitted your job scheduler (from where it submits more jobs).</p>"},{"location":"usage/#nextflow-memory-requirements","title":"Nextflow memory requirements","text":"<p>In some cases, the Nextflow Java virtual machines can start to request a large amount of memory. We recommend adding the following line to your environment to limit this (typically in <code>~/.bashrc</code> or <code>~./bash_profile</code>):</p> <pre><code>NXF_OPTS='-Xms1g -Xmx4g'\n</code></pre>"},{"location":"usage/customisation/","title":"Customisation","text":"<p>The viralmetagenome pipeline is highly customisable, allowing you to tailor the analysis to your specific needs. This section provides information on how to customise the pipeline, including configuration options and database customisation.</p>"},{"location":"usage/customisation/#custom-databases","title":"Custom Databases","text":"<p>Viralmetagenome uses a variety of databases to analyse reads, contigs, and consensus constructs. While the default databases are sufficient for most cases, you can create custom databases if needed. This section provides guidance on how to create and use custom databases for different tools used in the pipeline. For more details, see the databases guide.</p>"},{"location":"usage/customisation/#custom-tool-configuration","title":"Custom Tool Configuration","text":"<p>Viralmetagenome allows you to modify the arguments of the tools used in the pipeline by providing a custom configuration file. This can be done by copying a segment from the <code>modules.config</code> file and modifying the arguments as needed. For more details, see the configuration guide.</p>"},{"location":"usage/customisation/configuration/","title":"Custom configuration of modules","text":""},{"location":"usage/customisation/configuration/#using-argument_tool_name-parameters","title":"Using <code>--argument_tool_name</code> parameters","text":"<p>The viralmetagenome pipeline uses a set of tools to perform the analysis. Each tool has its own set of arguments that can be modified. The pipeline has a default configuration but this can be overwritten by supplying a custom configuration file. This file can be provided to viralmetagenome using the <code>--argument_tool_name</code> Nextflow option.</p> <p>For example, to change the minimum depth to call consensus to 5 and the minimum quality score of base to 30 for the <code>ivar consensus</code> module, we can use the <code>--ivar_consensus</code> parameter:</p> <pre><code>nextflow run nf-core/viralmetagenome \\\n    -profile docker \\\n    --arguments_ivar_consensus1 '-q 30 -m 5' \\\n    --arguments_ivar_consensus2 '--min-BQ 30' \\\n    --input samplesheet.csv ...\n</code></pre> <p>:::info This will overwrite all default arguments of viralmetagenome for the <code>ivar consensus</code> module. Similarly the remove the default values of viralmetagenome, specifiy the argument with an empty string:</p> <pre><code>nextflow run nf-core/viralmetagenome \\\n    -profile docker \\\n    --arguments_ivar_consensus1 '' \\\n    --arguments_ivar_consensus2 '' \\\n    --input samplesheet.csv ...\n</code></pre> <p>:::</p>"},{"location":"usage/customisation/configuration/#supplying-a-custom-configuration-file","title":"Supplying a custom configuration file","text":"<p>Within viralmetagenome, all modules (tools, e.g., <code>FASTP</code>, <code>FASTQC</code>) can be run with specific arguments. The pipeline has a default configuration but this can be overwritten by supplying a custom configuration file. This file can be provided to viralmetagenome using the <code>-c</code> Nextflow option.</p> <p>To see which specific arguments or variables are used for a module or tool, have a look at the <code>modules.config</code> file. Here the arguments of a module are specified as follows:</p> <pre><code>withName: IVAR_CONSENSUS {\n    ext.args = [\n        '-t 0.75',          // frequency to call consensus: 0.75 just the majority rule\n        '-q 20',            // minimum quality score of base\n        '-m 10',            // minimum depth to call consensus\n        '-n N'              // Character to print in regions with less coverage\n    ].join(' ').trim()\n    ext.args2 = [\n        '--count-orphans',  // Do not skip anomalous read pairs in variant calling.\n        '--max-depth 0',    // Maximum number of reads to start to consider at each location, 0 means no limit\n        '--min-BQ 20',      // Minimum base quality\n        '--no-BAQ',         // Disable probabilistic realignment for the computation of base alignment quality\n        '-aa',              // Output absolutely all positions, including unused reference sequences\n    ].join(' ').trim()\n    ...\n}\n</code></pre> <p>In this example, the <code>IVAR_CONSENSUS</code> module is configured with the arguments <code>-q 20 -m 10</code> for the tool <code>ivar consensus</code> and <code>--ignore-overlaps --count-orphans --max-depth 0 --no-BAQ --min-BQ 0</code> for <code>samtools mpileup</code> as iVar uses the output of <code>samtools mpileup</code> directly.</p> <p>:::tip The <code>ext.args</code> and <code>ext.args2</code> are used to specify the arguments for the tool. If unsure which tools use which arguments (<code>ivar:ext.args</code> and <code>samtools:ext.args2</code>), have a look at the nextflow module file directly! For example, at <code>modules/nf-core/ivar/consensus.nf</code>, \"\\(args\" and \"\\)args2\" are used to specify the arguments for the tools:</p> <pre><code>    \"\"\"\n    samtools \\\\\n        mpileup \\\\\n        --reference $fasta \\\\\n        $args2 \\\\               // can be modified with ext.args2\n        $bam \\\\\n        $mpileup \\\\\n        | ivar \\\\\n            consensus \\\\\n            $args \\\\            // can be modified with ext.args\n            -p $prefix\n    ...\n    \"\"\"\n</code></pre> <p>:::</p> <p>In case we do want to modify the arguments of a module, we can do so by providing a custom configuration file. The easiest way to do this would be to copy a segment from the modules.config and modify the arguments. This way, none of the other configurations will get lost or modified. For example, setting the minimum depth to call consensus to 5 and the minimum quality score of base to 30 for the <code>IVAR_CONSENSUS</code> module:</p> <p>```groovy title='custom.config' {5-6,12} process {     withName: IVAR_CONSENSUS {         ext.args = [             '-t 0.75',             '-q 30',            // changed             '-m 5',             // changed             '-n N'         ].join(' ').trim()         ext.args2 = [             '--count-orphans',             '--max-depth 0',             '--min-BQ 30',      // changed             '--no-BAQ',             '-aa',         ].join(' ').trim()         ...     } } <pre><code>:::warning\nMake sure you include the `process{}` section.\n:::\n\nNext, supply the file to viralmetagenome using the `-c` Nextflow option:\n\n```bash\nnextflow run nf-core/viralmetagenome \\\n    -profile docker \\\n    -c custom.config \\\n    --input samplesheet.csv ...\n</code></pre></p> <p>:::tip This guide not entirely clear? Also have a look at the nf-core guide for customizing tool arguments. :::</p>"},{"location":"usage/customisation/databases/","title":"Databases","text":""},{"location":"usage/customisation/databases/#introduction","title":"Introduction","text":"<p>Viralmetagenome uses a multitude of databases in order to analyze reads, contigs, and consensus constructs. The default databases will be sufficient in most cases but there are always exceptions. This document will guide you towards the right documentation location for creating your custom databases.</p> <p>:::tip Keep an eye out for nf-core createtaxdb as it can be used for the customization of the main databases but the pipeline is still under development. :::</p>"},{"location":"usage/customisation/databases/#reference-pool","title":"Reference pool","text":"<p>The reference pool dataset is used to identify potential references for scaffolding. It's a fasta file that will be used to make a blast database within the pipeline. The default database is the clustered Reference Viral DataBase (C-RVDB) a database that was built for enhancing virus detection using high-throughput/next-generation sequencing (HTS/NGS) technologies. An alternative reference pool is the Virosaurus database which is a manually curated database of viral genomes.</p> <p>Any nucleotide fasta file will do. Specify it with the parameter <code>--reference_pool</code>.</p>"},{"location":"usage/customisation/databases/#kaiju","title":"Kaiju","text":"<p>The Kaiju database will be used to classify the reads and intermediate contigs in taxonomic groups. The default database is the RVDB-prot pre-built database from Kaiju.</p> <p>A number of Kaiju pre-built indexes for reference datasets are maintained by the developers of Kaiju and made available on the Kaiju website. To build a Kaiju database, you need three components: a FASTA file with the protein sequences, the NCBI taxonomy dump files, and you need to define the uppercase characters of the standard 20 amino acids you wish to include.</p> <p>:::warning The headers of the protein fasta file must be numeric NCBI taxon identifiers of the protein sequences. :::</p> <p>To download the NCBI taxonomy files, please run the following commands:</p> <pre><code>wget https://ftp.ncbi.nlm.nih.gov/pub/taxonomy/new_taxdump/new_taxdump.zip\nunzip new_taxdump.zip\n</code></pre> <p>To build the database, run the following command (the contents of taxdump must be in the same location where you run the command):</p> <pre><code>kaiju-mkbwt -a ACDEFGHIKLMNPQRSTVWY -o proteins proteins.faa\nkaiju-mkfmi proteins\n</code></pre> <p>:::tip You can speed up database construction by supplying the threads parameter (<code>-t</code>). :::</p> <p>:::note{title=\"Expected files in database directory\" collapse}</p> <ul> <li><code>kaiju</code></li> <li><code>kaiju_db_*.fmi</code></li> <li><code>nodes.dmp</code></li> <li><code>names.dmp</code></li> </ul> <p>:::</p> <p>For the Kaiju database construction documentation, see here.</p>"},{"location":"usage/customisation/databases/#kraken2-databases","title":"Kraken2 databases","text":"<p>The Kraken2 database will be used to classify the reads and intermediate contigs in taxonomic groups.</p> <p>A number of database indexes have already been generated and maintained by @BenLangmead Lab, see here. These databases can directly be used to run the workflow with Kraken2 as well as Bracken.</p> <p>In case the databases above do not contain your desired libraries, you can build a custom Kraken2 database. This requires two components: a taxonomy (consisting of <code>names.dmp</code>, <code>nodes.dmp</code>, and <code>*accession2taxid</code>) files, and the FASTA files you wish to include.</p> <p>To pull the NCBI taxonomy, you can run the following:</p> <pre><code>kraken2-build --download-taxonomy --db &lt;YOUR_DB_NAME&gt;\n</code></pre> <p>You can then add your FASTA files with the following build command.</p> <pre><code>kraken2-build --add-to-library *.fna --db &lt;YOUR_DB_NAME&gt;\n</code></pre> <p>You can repeat this step multiple times to iteratively add more genomes prior to building.</p> <p>Once all genomes are added to the library, you can build the database (and optionally clean it up):</p> <pre><code>kraken2-build --build --db &lt;YOUR_DB_NAME&gt;\nkraken2-build --clean --db &lt;YOUR_DB_NAME&gt;\n</code></pre> <p>You can then add the <code>&lt;YOUR_DB_NAME&gt;/</code> path to your nf-core/taxprofiler database input sheet.</p> <p>:::tip{title=\"Expected files in database directory\"}</p> <ul> <li><code>kraken2</code></li> <li><code>opts.k2d</code></li> <li><code>hash.k2d</code></li> <li><code>taxo.k2d</code></li> </ul> <p>:::</p> <p>You can follow the Kraken2 tutorial for a more detailed description.</p>"},{"location":"usage/customisation/databases/#host-read-removal","title":"Host read removal","text":"<p>Viralmetagenome uses Kraken2 to remove contaminated reads.</p> <p>::: info \"Why kraken2 for host removal?\" The reason why we use Kraken2 for host removal over regular read mappers is nicely explained in the following papers:</p> <ul> <li>Benchmarking of Human Read Removal Strategies for Viral and Microbial Metagenomics</li> <li>The human \u201ccontaminome\u201d: bacterial, viral, and computational contamination in whole genome sequences from 1000 families</li> <li>Reconstruction of the personal information from human genome reads in gut metagenome sequencing data</li> </ul> <p>:::</p> <p>The contamination database is likely the largest database. The default databases are made small explicitly to save storage for end users but are not optimal. I would recommend creating a database consisting of the libraries <code>human, archaea, bacteria</code> which will be more than 200GB in size. Additionally, it's good practice to include DNA &amp; RNA of the host of origin if known (i.e. mice, ticks, mosquito, ... ). Add it as described above.</p> <p>Set it with the variable <code>--host_k2_db</code></p>"},{"location":"usage/customisation/databases/#viral-diversity-with-kraken2","title":"Viral Diversity with Kraken2","text":"<p>The metagenomic diversity estimated with Kraken2 is based on the viral refseq database which can cut short if you expect the species within your sample to have a large amount of diversity eg below 80% ANI (quasi-species). To resolve this it's better to create a database that contains a wider species diversity than only one genome per species. Databases that have this wider diversity are Virosaurus or the RVDB which can increase the accuracy of Kraken2. Add it as described above.</p> <p>Set it with the variable <code>--kraken2_db</code></p>"},{"location":"usage/customisation/databases/#annotation-sequences","title":"Annotation sequences","text":"<p>Identifying the species and the segment of the final genome constructs is done based on a tblastx search (with MMSEQS) to an annotated sequencing dataset. This dataset is by default the Virosaurus as it contains a good representation of the viral genomes and is annotated.</p> <p>This annotation database can be specified using <code>--annotation_db</code></p>"},{"location":"usage/customisation/databases/#creating-a-custom-annotation-dataset-with-bv-brc","title":"Creating a custom annotation dataset with BV-BRC","text":"<p>In case Virosaurus does not suffice your needs, a custom annotation dataset can be made. Creating a custom annotation dataset can easily be done as long as the annotation data is in the fasta header using this format: <code>(key)=(value)</code> or <code>(key):(value)</code>. For example, the following fasta headers are both valid:</p> <pre><code>&gt;754189.6 species=\"Ungulate tetraparvovirus 3\"|segment=\"nan\"|host_common_name=\"Pig\"|genbank_accessions=\"NC_038883\"|taxon_id=\"754189\"\n&gt;NC_001731; usual name=Molluscum contagiosum virus; clinical level=SPECIES; clinical typing=unknown; species=Molluscum contagiosum virus; taxid=10279; acronym=MOCV; nucleic acid=DNA; circular=N; segment=N/A; host=Human,Vertebrate;\n</code></pre> <p>An easy-to-use public database with a lot of metadata is BV-BRC. Sequences can be extracted using their CLI-tool and linked to their metadata</p> <p>Here we select all viral genomes that are not lab reassortments and are reference genomes and add metadata attributes to the output.</p> <p>This is an example, in case you need to have a more elaborate dataset than Virosaurus, be more inclusive towards your taxa of interest and include more metadata attributes.</p> <pre><code># download annotation metadata +/- 5s\np3-all-genomes --eq superkingdom,Viruses --eq reference_genome,Reference --ne host_common_name,'Lab reassortment' --attr genome_id,species,segment,genome_name,genome_length,host_common_name,genbank_accessions,taxon_id   &gt; all-virus-anno.txt\n# download genome data, done separately as it takes much longer to query +/- 1 hour\np3-all-genomes --eq superkingdom,Viruses --eq reference_genome,Reference --ne host_common_name,'Lab reassortment' | p3-get-genome-contigs --attr sequence &gt; all-virus.fasta\n</code></pre> <p>:::tip Any attribute can be downloaded and will be added to the final report if the formatting remains the same. For a complete list of attributes see <code>p3-all-genomes --fields</code> or read their manual :::</p> <p>Next, the metadata and the genomic data are combined into a single fasta file where the metadata fields are stored in the fasta comment as <code>key1=\"value1\"|key2=\"value2\"|...</code> using the following python code.</p> <pre><code>import pandas as pd\nimport re\n\n# read in sequences with all columns as strings\nsequences = pd.read_csv(\"refseq-virus.fasta\", index_col=0, sep=\"\\t\", dtype=str)\ndata = pd.read_csv(\"refseq-virus-anno.txt\", index_col=0, sep=\"\\t\", dtype=str)\n\n# merge the df's\ndf = sequences.join(data)\n# remove 'genome' from the name\ndf.columns = df.columns.str.replace(\"genome.\", \"\")\n\n# create fasta header\ndef create_fasta_header(row):\n    annotations = \";\".join(\n        [\n            f'{column}=\"{value}\"'\n            for column, value in row.items()\n            if column != \"contig.sequence\"\n        ]\n    )\n    return f\"{annotations}\\n\"\n\n\ndf[\"fasta_header\"] = df.apply(create_fasta_header, axis=1)\n\ndf[\"fasta_entry\"] = (\n    \"&gt;\" + df.index.astype(str) + \" \" + df[\"fasta_header\"] + df[\"contig.sequence\"]\n)\nwith open(\"bv-brc-refvirus-anno.fasta\", \"w\") as f:\n    for entry in df[\"fasta_entry\"]:\n        f.write(entry + \"\\n\")\n</code></pre> <p>:::tip{title=\"Expected files in database directory\"}</p> <ul> <li><code>refseq-virus.fasta</code></li> <li><code>refseq-virus-anno.txt</code></li> <li><code>bv-brc-refvirus-anno.fasta</code></li> </ul> <p>:::</p>"},{"location":"usage/workflow/","title":"Workflow overview","text":"<p>Viralmetagenome takes in a set of reads and performs 5 major analyses, each of them are explained in more detail in the following sections:</p> <ol> <li>Preprocessing</li> <li>Metagenomic diversity</li> <li>Assembly &amp; Polishing</li> <li>Variant analysis &amp; iterative refinement</li> <li>Consensus evaluation</li> </ol> <p>By default all analyses are run.</p> <p>:::tip{title=\"Skipping steps\"} All steps can be skipped and the pipeline can be run with only the desired steps. This can be done with the <code>--skip_preprocessing</code>, <code>--skip_read_classification</code>, <code>--skip_assembly</code>, <code>--skip_polishing</code>, <code>--skip_variant_analysis</code>, <code>--skip_iterative_refinement</code>, <code>--skip_consensus_qc</code> flags. :::</p>"},{"location":"usage/workflow/#subway-map","title":"Subway map","text":""},{"location":"usage/workflow/assembly_polishing/","title":"Assembly & polishing","text":"<p>Viralmetagenome offers an elaborate workflow for the assembly and polishing of viral genomes:</p> <ol> <li>Assembly: combining the results of multiple assemblers.</li> <li>Extension: extending contigs using paired-end reads.</li> <li>Coverage calculation: mapping reads back to the contigs to determine coverage.</li> <li>Reference Matching: comparing contigs to a reference sequence pool.</li> <li>Taxonomy guided Clustering: clustering contigs based on taxonomy and nucleotide similarity.</li> <li>Pre-clustering: separating contigs based on identified taxonomy-id.</li> <li>Actual clustering: clustering contigs based on nucleotide similarity.</li> <li>Scaffolding: scaffolding the contigs to the centroid of each bin.</li> <li>Annotation with Reference: annotating regions with 0-depth coverage with the reference sequence.</li> </ol> <p></p> <p>The overall workflow of creating reference assisted assemblies can be skipped with the argument <code>--skip_assembly</code>. See the parameters assembly section for all relevant arguments to control the assembly steps.</p> <p>The overall refinement of contigs can be skipped with the argument <code>--skip_polishing</code>. See the parameters polishing section for all relevant arguments to control the polishing steps.</p> <p>The consensus genome of all clusters are then sent to the variant analysis &amp; iterative refinement step.</p>"},{"location":"usage/workflow/assembly_polishing/#1-de-novo-assembly","title":"1. De-novo Assembly","text":"<p>Three assemblers are used, SPAdes, Megahit, and Trinity. The resulting contigs of all specified assemblers, are combined and processed further together.</p> <p>Modify the spades mode with <code>--spades_mode [default: rnaviral]</code> and supply specific params with <code>--spades_yml</code> or a hmm model with <code>--spades_hmm</code>.</p> <p>Specify the assemblers to use with the <code>--assemblers</code> parameter where the assemblers are separated with a ','. The default is <code>spades,megahit,trinity</code>.</p> <p>Low complexity contigs can be filtered out using prinseq++ with the <code>--skip_contig_prinseq false</code> parameter. Complexity filtering is primarily a run-time optimisation step. Low-complexity sequences are defined as having commonly found stretches of nucleotides with limited information content (e.g. the dinucleotide repeat CACACACACA). Such sequences can produce a large number of high-scoring but biologically insignificant results in database searches. Removing these reads therefore saves computational time and resources.</p>"},{"location":"usage/workflow/assembly_polishing/#2-extension","title":"2. Extension","text":"<p>Contigs can be extended using SSPACE Basic with the <code>--skip_sspace_basic false</code> parameter. SSPACE is a tool for scaffolding contigs using paired-end reads. It is modified from SSAKE assembler and has the feature of extending contigs using reads that are unmappable in the contig assembly step. To maximize its efficiency, consider specifying the arguments <code>--read_distance</code>, <code>--read_distance_sd</code>, and <code>--read_orientation</code>. For more information on these arguments, see the parameters assembly section.</p> <p>The extension of contigs is run by default, to skip this step, use <code>--skip_sspace_basic</code>.</p>"},{"location":"usage/workflow/assembly_polishing/#3-coverage-calculation","title":"3. Coverage calculation","text":"<p>Processed reads are mapped back against the contigs to determine the number of reads mapping towards each contig. This is done with <code>BowTie2</code>,<code>BWAmem2</code> or <code>BWA</code>. This step is used to remove contig clusters that have little to no coverage downstream.</p> <p>Specify the mapper to use with the <code>--mapper</code> parameter. The default is <code>BWAmem2</code>. To skip contig filtering specify <code>--perc_reads_contig 0</code>.</p>"},{"location":"usage/workflow/assembly_polishing/#4-reference-matching","title":"4. Reference Matching","text":"<p>The newly assembled contigs are compared to a reference sequence pool (<code>--reference_pool</code>) using a BLASTn search. This process not only helps annotate the contigs but also assists in linking together sets of contigs that are distant within a single genome. Essentially, it aids in identifying contigs belonging to the same genomic segment and choosing the right reference for scaffolding purposes.</p> <p>The top 5 hits for each contig are combined with the de novo contigs and sent to the clustering step.</p> <p>The reference pool can be specified with the <code>--reference_pool</code> parameter. The default is the latest clustered Reference Viral DataBase (RVDB).</p>"},{"location":"usage/workflow/assembly_polishing/#5-taxonomy-guided-clustering","title":"5. Taxonomy guided Clustering","text":"<p>The clustering workflow of contigs consists of 2 steps, the pre-clustering using taxonomy and actual clustering on nucleotide similarity. The taxonomy guided clustering is used to separate contigs based on taxonomy and nucleotide similarity.</p> <pre><code>graph LR;\n    A[Contigs] --&gt; B[\"`**Pre-clustering**`\"];\n    B --&gt; C[\"`**Actual clustering**`\"];</code></pre>"},{"location":"usage/workflow/assembly_polishing/#51-pre-clustering-using-taxonomy","title":"5.1 Pre-clustering using taxonomy","text":"<p>The contigs along with their references have their taxonomy assigned using Kraken2 and Kaiju.</p> <p>The default databases are the same ones used for read classification:</p> <ul> <li>Kraken2: viral refseq database, <code>--kraken2_db</code></li> <li>Kaiju: clustered RVDB, <code>--kaiju_db</code></li> </ul> <p>As Kaiju and Kraken2 can have different taxonomic assignments, an additional step is performed to resolve potential inconsistencies in taxonomy and to identify the taxonomy of the contigs. This is done with a custom script that is based on <code>KrakenTools extract_kraken_reads.py</code> and <code>kaiju-Merge-Outputs</code>.</p> <pre><code>graph LR;\n    A[Contigs] --&gt; B[\"`**Kraken2**`\"];\n    A --&gt; C[\"`**Kaiju**`\"];\n    B --&gt; D[Taxon merge resolving];\n    C --&gt; D;\n    D --&gt; E[\"Taxon filtering\"];\n    E --&gt; F[\"Taxon simplification\"];</code></pre> <p>:::tip{title=\"Having complex metagenomic samples?\"} The pre-clustering step can be used to simplify the taxonomy of the contigs, let NCBI's taxonomy browser help you identify taxon-id's for simplification. The simplification can be done in several ways:</p> <ul> <li>Make sure your contamination database is up to date and removes the relevant taxa.</li> <li>Exclude unclassified contigs with <code>--arguments_extract_precluster \"--keep-unclassified false\"</code> parameter.</li> <li>Simplify the taxonomy of the contigs to a higher rank using <code>--arguments_extract_precluster \"--precluster-simplify-taxa &lt;value&gt;\"</code> parameter (1).</li> <li>Specify the taxa to include or exclude with <code>--arguments_extract_precluster \"--precluster-include-children &lt;taxa&gt;\"</code>, <code>--arguments_extract_precluster \"--precluster-include-parents &lt;taxa&gt;\"</code>, <code>--arguments_extract_precluster \"--precluster-exclude-children &lt;taxa&gt;\"</code>, <code>--arguments_extract_precluster \"--precluster-exclude-parents &lt;taxa&gt;\"</code>, <code>--arguments_extract_precluster \"--precluster-exclude-taxa &lt;taxa&gt;\"</code> parameters.</li> </ul> <p>:::warning Providing lists to the extract precluster script is done by encapsulating values with <code>\"</code> and separating them with a space. For example: <code>--arguments_extract_precluster \"--precluster-exclude-taxa taxon1 taxon2 taxon3\"</code>. ::: :::</p> <ol> <li> <p>Options here are 'species', 'genus', 'family', 'order', 'class', 'phylum', 'kingdom' or 'superkingdom'.</p> </li> <li> <p><code>--precluster_include_children</code> \"genus1\" :</p> </li> </ol> <pre><code>graph TD;\n    A[family] -.- B[\"genus1 (included)\"];\n    A -.- C[genus2];\n    B --- D[species1];\n    B --- E[species2];\n    C -.- F[species3];</code></pre> <p>Dotted lines represent exclusion of taxa.</p> <ol> <li><code>--precluster_include_parents</code> \"species3\" :</li> </ol> <pre><code>graph TD;\n    A[\"family (included)\"] -.- B[\"genus1\"]\n    A --- C[genus2]\n    B -.- D[species1]\n    B -.- E[species2]\n    C --- F[species3]</code></pre> <p>Dotted lines represent exclusion of taxa.</p> <p>The pre-clustering step will be run by default but can be skipped with the argument <code>--skip_preclustering</code>. Specify which classifier to use with <code>--precluster_classifiers</code> parameter. The default is <code>kaiju,kraken2</code>. Contig taxon filtering is still enabled despite not having to solve for inconsistencies if only Kaiju or Kraken2 is run.</p>"},{"location":"usage/workflow/assembly_polishing/#52-actual-clustering-on-nucleotide-similarity","title":"5.2 Actual clustering on nucleotide similarity","text":"<p>The clustering is performed with one of the following tools:</p> <ul> <li><code>cdhitest</code></li> <li><code>vsearch</code></li> <li><code>mmseqs-linclust</code></li> <li><code>mmseqs-cluster</code></li> <li><code>vRhyme</code></li> <li><code>mash</code></li> </ul> <p>These methods all come with their own advantages and disadvantages. For example, cdhitest is very fast but cannot be used for large viruses &gt;10Mb and similarity threshold cannot go below 80% which is not preferable for highly diverse RNA viruses. Vsearch is slower but accurate. Mmseqs-linclust is the fastest but tends to create a large amount of bins. Mmseqs-cluster is slower but can handle larger datasets and is more accurate. vRhyme is a new method that is still under development but has shown promising results but can sometimes not output any bins when segments are small. Mash is a very fast comparison method is linked with a custom script that identifies communities within a network.</p> <p>:::tip When pre-clustering is performed, it is recommended to set a lower identity_threshold (60-70% ANI) as the new goal becomes to separate genome segments within the same bin. :::</p> <p>The clustering method can be specified with the <code>--clustering_method</code> parameter. The default is <code>cdhitest</code>.</p> <p>The network clustering method for <code>mash</code> can be specified with the <code>--network_clustering</code> parameter. The default is <code>connected_components</code>, alternative is <code>leiden</code>.</p> <p>The similarity threshold can be specified with the <code>--similarity_threshold</code> parameter. The default is <code>0.85</code>.</p>"},{"location":"usage/workflow/assembly_polishing/#6-coverage-filtering","title":"6. Coverage filtering","text":"<p>The coverage of the contigs is calculated using the same method as in the coverage calculation step. A cumulative sum is taken across the contigs from every assembler. If these cumulative sums are above the specified <code>--perc_reads_contig</code> parameter, the contig is kept. If all cumulative sums are below the specified parameter, the contig is removed.</p> <p>:::info{title=\"Show me an example how it works\"} If the <code>--perc_reads_contig</code> is set to <code>5</code>, the cumulative sum of the contigs from every assembler is calculated. For example:</p> <ul> <li>Cluster 1: the cumulative sum of the contigs from SPAdes is 0.6, Megahit is 0.5, the cluster is kept.</li> <li>Cluster 2: the cumulative sum of the contigs from SPAdes is 0.1, Megahit is 0.1, the cluster is removed.</li> <li>Cluster 3: the cumulative sum of the contigs from SPAdes is 0.5, Megahit is 0, the cluster is kept.</li> </ul> <p>:::</p> <p>The default is <code>5</code> and can be specified with the <code>--perc_reads_contig</code> parameter.</p>"},{"location":"usage/workflow/assembly_polishing/#7-scaffolding","title":"7. Scaffolding","text":"<p>After classifying all contigs and their top BLAST hits into distinct clusters or bins, the contigs are then scaffolded to the centroid of each bin. Any external references that are not centroids of the cluster are subsequently removed to prevent further bias. All members of the cluster are consequently mapped towards their centroid with Minimap2 and consensus is called using iVar-consensus.</p>"},{"location":"usage/workflow/assembly_polishing/#8-annotation-with-reference","title":"8. Annotation with Reference","text":"<p>Regions with 0-depth coverage are annotated with the reference sequence. This is done with a custom script that uses the coverage of the de novo contigs towards the reference sequence to identify regions with 0-depth coverage. The reference sequence is then annotated to these regions.</p> <p>This step can be skipped using <code>--skip_nocov_to_reference</code> parameter.</p>"},{"location":"usage/workflow/consensus_qc/","title":"Report generation and quality control","text":"<p>Viralmetagenome's report and result interpretation heavily relies on MultiQC. MultiQC is a tool to create a single report from multiple analysis results. It is designed to be used with a wide range of bioinformatics tools and is compatible with a wide range of data formats. Almost all tools are summarised within the MultiQC report that have interactive plots and data tables. However, due to the number of tools included, some results are summarised in the directory <code>overview-tables</code> to reduce the size of the MultiQC report.</p> <p>:::tip Complete output descriptions of files and images can be found in the output section. :::</p> <p>Within the MultiQC report, Viralmetagenome provides a number of custom tables based on consensus genome quality control data. These tools are:</p> <ul> <li>QUAST: QUAST is a quality assessment tool for genome assemblies. It calculates various metrics such as N50, L50, number of contigs, and total length.</li> <li>CheckV: CheckV is a tool for assessing the quality of metagenome-assembled viral genomes. It calculates various metrics such as completeness, contamination, and strain heterogeneity.</li> <li>Prokka: Prokka is a whole genome annotation pipeline for identifying features of interest in a set of genomic DNA sequences, and labelling them with useful information.</li> <li>blastn: BLAST is a tool for comparing primary biological sequence information. It calculates the similarity between the consensus genome and the reference genome.</li> <li>mmseqs-search - included as 'annotation': MMseqs is an ultra-fast and sensitive search tool for protein and nucleotide databases. Viralmetagenome uses MMseqs to annotate the consensus genomes and assign them a species name, segment name, expected host, etc.</li> <li>mafft: MAFFT is a multiple sequence alignment program.</li> <li>SnpEff and SnpSift: SnpEff is a genetic variant annotation and functional effect prediction tool. SnpSift is a toolbox that allows you to filter and manipulate annotated files.</li> </ul> <p>Consensus genome quality control can be skipped with <code>--skip_consensus_qc</code>.</p>"},{"location":"usage/workflow/consensus_qc/#quast","title":"QUAST","text":"<p>QUAST is a quality assessment tool for genome assemblies. It calculates various metrics such as N50, L50, number of contigs, and total length. However, in the summary table, it is mainly used to get the number of ambiguous bases in the consensus genome.</p> <p>QUAST can be skipped with <code>--skip_quast</code>.</p>"},{"location":"usage/workflow/consensus_qc/#checkv","title":"CheckV","text":"<p>CheckV is a tool for assessing the quality of metagenome-assembled viral genomes. It calculates various metrics such as completeness, contamination, and strain heterogeneity. CheckV estimates completeness by comparing sequences with a large database of complete viral genomes, metagenomes, metatranscriptomes, and metaviromes.</p> <p>:::tip{title=\"Incomplete genomes for segmented viruses\"} CheckV estimates the completeness of a virus based on all genome segments. If a virus has multiple segments, the completeness of the virus is calculated based on the length of the concatenated segments. For example, Lassa virus has 2 segments L: 7.2kb and S: 3.4kb. The completeness of the virus is calculated based on the length of the concatenated segments (7.2kb + 3.4kb = 10.6kb) and so if the generated consensus genome of the L segment is 7.1kb it will report the completeness as 7.1/10.6 ~ 67%. :::</p> <p>CheckV can be skipped with <code>--skip_checkv</code>.</p>"},{"location":"usage/workflow/consensus_qc/#prokka","title":"Prokka","text":"<p>Prokka is a whole genome annotation pipeline for identifying features of interest in a set of genomic DNA sequences, and labelling them with useful information. Prokka is a software tool to annotate bacterial, archaeal and viral genomes.</p> <p>:::tip{title=\"Suboptimal annotation\"} Prokka was initially designed for bacterial and archaeal genomes, and may not be optimal for viral genomes. VIGOR4 is a good alternative but is species specific. :::</p> <p>:::tip{title=\"Custom protein database\"} Prokka can be given a custom protein database to annotate your genomes with, have a look at prot-RVDB for viral protein databases. Supply the database using <code>--prokka_db</code>. :::</p> <p>Prokka can be skipped with <code>--skip_prokka</code>.</p>"},{"location":"usage/workflow/consensus_qc/#blast","title":"BLAST","text":"<p>blastn is a tool for comparing primary biological sequence information. It calculates the similarity between the consensus genome and the reference genome. The similarity is calculated based on the number of identical bases between the two sequences. Viralmetagenome uses blastn to compare the sequences against the supplied <code>--reference_pool</code> dataset.</p> <p>BLASTn can be skipped with <code>--skip_blast_qc</code>.</p>"},{"location":"usage/workflow/consensus_qc/#mmseqs-search","title":"MMseqs-search","text":"<p>MMseqs-search is an ultra-fast and sensitive search tool for protein and nucleotide databases. Viralmetagenome uses MMseqs to search the consensus genomes in an annotated database, like Virousarus (see also defining your own custom annotation database), and uses the annotation data of the best hit to assign the consensus genome a species name, segment name, expected host, and any other metadata that is embedded within the database. This allows Viralmetagenome, in addition to the BLAST search of reference pool hits, to compare the generated consensus genomes at a species &amp; segment level.</p> <p>:::info MMseqs was used for the annotation step instead of BLAST because of the ability to query using a tblastx search for highly diverging viruses while supplying a nucleotide annotation database. To specify another type of search (e.g. blastp, blastx, etc.), please refer to the parameters consensus-qc section. :::</p> <p>MMseqs-search can be skipped with <code>--skip_consensus_annotation</code>.</p>"},{"location":"usage/workflow/consensus_qc/#snpeff-and-snpsift","title":"SnpEff and SnpSift","text":"<p>SnpEff is a genetic variant annotation and functional effect prediction tool. It annotates and predicts the effects of genetic variants on genes and proteins (such as amino acid changes).</p> <p>SnpSift is a toolbox that allows you to filter and manipulate annotated files. The ExtractFields tool is used to extract specific information from the annotated VCF files into a tabular format for easier analysis.</p> <p>Viralgenie uses SnpEff to annotate variants identified by the variant calling process with functional information, and SnpSift ExtractFields to extract key information from the annotated variants into a more accessible tabular format.</p> <p>The annotation process provides valuable information about the impact of variants, including:</p> <ul> <li>Whether variants are synonymous or non-synonymous</li> <li>Changes in amino acid sequences</li> <li>Potential impact severity (HIGH, MODERATE, LOW, MODIFIER)</li> <li>Gene and transcript information</li> </ul> <p>Variant annotation can be skipped with <code>--skip_vcf_annotation</code>.</p>"},{"location":"usage/workflow/consensus_qc/#mafft","title":"MAFFT","text":"<p>MAFFT is a multiple sequence alignment program. It is used to align the following genomic data:</p> <ul> <li>The final consensus genome</li> <li>The identified reference genome from <code>--reference_pool</code></li> <li>The de novo contigs from each assembler (that constituted the final consensus genome)</li> <li>Each consensus genome from the iterative refinement steps.</li> </ul> <p>MAFFT can be skipped with <code>--skip_alignment_qc</code>.</p>"},{"location":"usage/workflow/consensus_qc/#multiqc","title":"MultiQC","text":"<p>MultiQC is a tool to create a single report with interactive plots for multiple bioinformatics analyses across many samples.</p> <p></p> <p>Reports are generated by scanning given directories for recognised log files. These are parsed and a single HTML report is generated summarising the statistics for all logs found. MultiQC reports can describe multiple analysis steps and large numbers of samples within a single plot, and multiple analysis tools making it ideal for routine fast quality control.</p> <p>MultiQC is also used to generate the <code>overview-tables</code> as it extracts the additional data from various tools. The data that needs to be extracted can be modified with the argument <code>--custom_table_headers</code> where a yml file shows which tools need to be included in the summary table in addition to BLAST, CheckV, QUAST, and MMseqs (annotation).</p> custom_table_headers.yml<pre><code>tool:\n  - Tool subsection: # if applicable\n      - name_in_mqc_table: \"new name\"\n      - output_reads: \"deduplicated reads\"\n      - percent_passing_dedup: \"% passing dedup\"\n</code></pre>"},{"location":"usage/workflow/metagenomic_diversity/","title":"Read classification","text":"<p>Viralmetagenome offers two main tools for the classification of reads and a summary visualisation tool:</p> <ul> <li>Kaiju: Taxonomic classification based on maximum exact matches using protein alignments.</li> <li>Kraken2: Assigns taxonomic labels on a DNA level using a k-mer approach. (optional Bracken)</li> <li>Krona: Interactive multi-layered pie charts of hierarchical data.</li> </ul> <p></p> <p>:::tip{title=\"Want more classifiers?\"} Feel free to reach out and suggest more classifiers. However, if the main goal of your project is to establish the presence of a virus within a sample and are therefore only focused on metagenomic diversity, have a look at taxprofiler :::</p> <p>The read classification can be skipped with the argument <code>--skip_read_classification</code>, classifiers should be specified with the parameter <code>--read_classifiers 'kaiju,kraken2'</code> (no spaces, no caps). See the parameters classification section for all relevant arguments to control the classification steps.</p>"},{"location":"usage/workflow/metagenomic_diversity/#kaiju","title":"Kaiju","text":"<p>Kaiju classifies individual metagenomic reads using a reference database comprising the annotated protein-coding genes of a set of microbial genomes. It employs a search strategy, which finds maximal exact matching substrings between query and database using a modified version of the backwards search algorithm in the Burrows-Wheeler transform. The Burrows-Wheeler transform is a text transformation that converts the reference sequence database into an easily searchable representation, which allows for exact string matching between a query sequence and the database in time proportional to the length of the query.</p>"},{"location":"usage/workflow/metagenomic_diversity/#kraken2","title":"Kraken2","text":"<p>Kraken2 is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.</p> <p>Bracken can optionally be enabled for more accurate estimation of abundances, although these values should be interpreted with caution as viruses don't have marker genes making it difficult to compare abundances across samples &amp; taxa. <code>--read_classifiers 'kraken2,bracken'</code> (no spaces, no caps)</p>"},{"location":"usage/workflow/metagenomic_diversity/#krona","title":"Krona","text":"<p>Krona allows hierarchical data to be explored with zooming, multi-layered pie charts. The interactive charts are self-contained and can be viewed with any modern web browser.</p>"},{"location":"usage/workflow/preprocessing/","title":"Preprocessing","text":"<p>Viralmetagenome offers three main preprocessing steps for the preprocessing of raw sequencing reads:</p> <ol> <li>Adapter trimming: adapter clipping and pair-merging.</li> <li>UMI deduplication: removal of PCR duplicates based on Unique Molecular Identifiers (UMIs) on a read level.</li> <li>Read merging: merging of reads from the same sample.</li> <li>Complexity filtering: removal of low-sequence complexity reads.</li> <li>Host read-removal: removal of reads aligning to reference genome(s) of a host.</li> </ol> <p></p> <p>Preprocessing can be entirely skipped with the option <code>--skip_preprocessing</code>. See the parameters preprocessing section for all relevant arguments to control the preprocessing steps.</p> <p>:::tip Samples with fewer than <code>--min_trimmed_reads [default: 1]</code> reads will be removed from any further downstream analysis. These samples will be highlighted in the MultiQC report. :::</p>"},{"location":"usage/workflow/preprocessing/#read-quality-control","title":"Read Quality control","text":"<p><code>FastQC</code> gives general quality metrics about your reads. It provides information about the quality score distribution across your reads, per base sequence content (%A/T/G/C), adapter contamination, and overrepresented sequences. <code>FastQC</code> is used before and after read processing and after host read-removal to assess the quality of the reads.</p> <pre><code>graph LR;\n    A[Raw reads] --&gt; B[\"`**FastQC**`\"];\n    B --&gt; C[Read processing];\n    C --&gt; D[\"`**FastQC**`\"];\n    D --&gt; E[UMI deduplication];\n    E --&gt; G[Complexity filtering];\n    G --&gt; H[Host read-removal];\n    H --&gt; I[\"`**FastQC**`\"];</code></pre>"},{"location":"usage/workflow/preprocessing/#1-adapter-trimming","title":"1. Adapter trimming","text":"<p>Raw sequencing read processing in the form of adapter clipping and paired-end read merging is performed by the tools <code>fastp</code> or <code>Trimmomatic</code>. The tool <code>fastp</code> is a fast all-in-one tool for preprocessing fastq files. The tool <code>Trimmomatic</code> is a flexible read trimming tool for Illumina NGS data. Both tools can be used to remove adapters and low-quality reads from the raw sequencing reads. An adapter file can be provided through the argument <code>--adapter_fasta</code>.</p> <p>Specify the tool to use for read processing with the <code>--trim_tool</code> parameter, the default is <code>fastp</code>.</p>"},{"location":"usage/workflow/preprocessing/#2-umi-deduplication","title":"2. UMI deduplication","text":"<p>Unique Molecular Identifiers (UMIs) are short sequences that are added during library preparation. They are used to identify and remove PCR duplicates. The tool <code>HUMID</code> is used to remove PCR duplicates based on the UMI sequences. HUMID supports two ways to group reads using their UMI. By default, HUMID uses the directional method, which takes into account the expected errors based on the PCR process. Specify the allowed amount of errors to see reads coming from the same original fragment with <code>--arguments_humid '-m 5'</code>, for a distance of 5 [default : 1]. Alternatively, HUMID supports the maximum clustering method, where all reads that are within the specified distance are grouped together.</p> <p>:::tip{title=\"Directional vs maximum clustering\"} </p> <p>_Taken from [UMI-tools: 'The network based deduplication methods'](https://umi-tools.readthedocs.io/en/latest/the_methods.html)_ </p> <ul> <li>cluster: Form networks of connected UMIs with a mismatch distance of 1. Each connected component is a read group. In the above example, all the UMIs are contained in a single connected component and thus there is one read group containing all reads, with ACGT as the \u2018selected\u2019 UMI.</li> <li>directional (default for both HUMID and UMI-tools): Form networks with edges defined based on distance threshold and $$ \\text{ node A counts} \\geq (2 \\cdot \\text{node B counts}) - 1$$   Each connected component is a read group, with the node with the highest counts selected as the top node for the component. In the example above, the directional edges yield two connected components. One with AAAT by itself and the other with the remaining UMIs with ACGT as the selected node.</li> </ul> <p>:::</p> <p>Viralmetagenome supports both deduplication on a read level as well as a mapping level. Specify the <code>--umi_deduplication</code> with <code>read</code> or <code>mapping</code> to choose between the two or specify <code>both</code> to both deduplicate on a read level as well as on a mapping level (after read mapping with reference).</p> <p>By default, viralmetagenome doesn't assume UMIs are present in the reads. If UMIs are present, specify the <code>--with_umi</code> parameter and <code>--deduplicate</code>.</p>"},{"location":"usage/workflow/preprocessing/#3-read-merging","title":"3. Read merging","text":"<p>Certain patients or original samples can be sequenced in multiple times. This step involves merging reads from these different sequencing methods to create a comprehensive dataset for analysis.</p> <p>only R1 will be merged with R1 and R2 with R2. Single end and paired end reads will not be merged.</p> <p>This is done with <code>CAT</code>.</p>"},{"location":"usage/workflow/preprocessing/#4-complexity-filtering","title":"4. Complexity filtering","text":"<p>Complexity filtering is primarily a run-time optimization step. Low-complexity sequences are defined as having commonly found stretches of nucleotides with limited information content (e.g., the dinucleotide repeat CACACACACA). Such sequences can produce a large number of high-scoring but biologically insignificant results in database searches. Removing these reads therefore saves computational time and resources.</p> <p>Complexity filtering is done with <code>Bbduk</code> which is part of <code>BBtools</code> where the \"duk\" stands for Decontamination Using Kmers. Alternatively, complexity filtering can be done with <code>prinseq++</code>.</p> <p>By default, this step is skipped. If this step shouldn't be skipped, specify <code>--skip_complexity_filtering false</code>. Specify the tool to use for complexity filtering with the <code>--decomplexifier</code> parameter, <code>bbduk</code> or <code>prinseq</code> [default].</p>"},{"location":"usage/workflow/preprocessing/#5-host-read-removal","title":"5. Host read-removal","text":"<p>Contamination, whether derived from experiments or computational processes, looms large in next-generation sequencing data. Such contamination can compromise results from WGS as well as metagenomics studies, and can even lead to the inadvertent disclosure of personal information. To avoid this, host read-removal is performed. Host read-removal is performed by the tool <code>Kraken2</code>.</p> <p>::: info \"Interesting reads\" The reason why we use Kraken2 for host removal over regular read mappers is nicely explained in the following papers:</p> <ul> <li>Benchmarking of Human Read Removal Strategies for Viral and Microbial Metagenomics</li> <li>The human \u201ccontaminome\u201d: bacterial, viral, and computational contamination in whole genome sequences from 1000 families</li> <li>Reconstruction of the personal information from human genome reads in gut metagenome sequencing data</li> </ul> <p>:::</p> <p>Specify the host database with the <code>--host_k2_db</code> parameter. The default is a small subset of the human genome and we highly suggest that you make this database more elaborate (for example, complete human genome, common sequencer contaminants, bacterial genomes, ...). For this, read the section on creating custom kraken2 host databases.</p>"},{"location":"usage/workflow/variant_and_refinement/","title":"Variant calling and consensus refinement","text":"<p>This subworkflow supports two distinct starting points:</p>"},{"location":"usage/workflow/variant_and_refinement/#a-external-reference-based-analysis","title":"A. External Reference-based Analysis:","text":"<p>References are provided through the samplesheet <code>--mapping_constraints</code></p> <ol> <li>Best matching reference genome is selected</li> <li>Reads are mapped to these reference genome(s)</li> <li>Variants are called from the mappings</li> <li>A consensus genome is generated based on the variant calls</li> </ol>"},{"location":"usage/workflow/variant_and_refinement/#b-de-novo-assembly-refinement","title":"B. De novo Assembly Refinement","text":"<p>Uses the (reference-assisted) de novo consensus genomes as input Performs iterative refinement:</p> <ol> <li>Uses current consensus as reference</li> <li>Maps reads back to this reference</li> <li>Variants are called from the mappings</li> <li>A consensus genome is generated based on the variant calls</li> <li>Repeats steps 1-4 for specified number of iterations (default: 2)</li> </ol> <p>Both approaches use the same variant calling and consensus generation methods, but differ in their starting point and purpose.</p> <p></p> <p>:::info This schema is a simplification as there are some additional steps:</p> <ul> <li>Deduplication: (optional) deduplication of reads can be performed with <code>Picard</code> or if UMIs are used <code>UMI-tools</code>.</li> <li>Variant filtering: variant filtering, only variants with sufficient depth and quality are retained for consensus calling (only for BCFtools).</li> <li>Mapping statistics: (optional) generate multiple summary statistics of the BAM files.</li> </ul> <p>:::</p> <p>:::info The variant calling and consensus refinement step can be skipped with the argument <code>--skip_iterative_refinement</code> and <code>--skip_variant_calling</code>, see the parameters iterative refinement section and parameters variant analysis section, respectively, for all relevant arguments to control the variant analysis steps. :::</p>"},{"location":"usage/workflow/variant_and_refinement/#1a-selection-of-reference","title":"1a. Selection of reference","text":"<p>The reference genome(s) can be supplied with the samplesheet <code>--mapping_constraints</code>, here the reference can be a multiFasta file representing a range of genomes that could be valid reference genomes. Here, viralmetagenome supports a selection procedure where the reference genomes that share the highest number of k-mers with the read files will be selected and kept for read mapping, variant calling and consensus genome reconstruction.</p> <pre><code>graph LR\n    A[reference genomes] --&gt; B[Sketching]\n    B --&gt; C[Distance evaluation]\n    D[Reads] --&gt; C\n    C --&gt; E[Reference selection]</code></pre> <p>:::tip As with any mapping tool, the reference genome(s) should be as close as possible to the sample genome(s) to avoid mapping bias, especially for fast mutating viruses. If the reference genome is too different from the sample genome, the reads will likely not map correctly and could result in incorrect variants and consensus. For this reason, use an extensive reference dataset like the RVDB, if possible even the unclustered one. :::</p> <p>This procedure is done with <code>Mash</code> where the reads are compared to the reference genomes and the reference genome with the highest number of shared k-mers is selected. The number of shared k-mers can be specified with <code>--arguments_mash_dist \"-k 15\"</code> (default k-mer size: <code>15</code>), and the number of sketches to create with <code>--arguments_mash_dist \"-s 4000\"</code> (default sketch size: <code>4000</code>). These parameters can be combined as <code>--arguments_mash_dist \"-s 4000 -k 15\"</code>.</p> <p>:::tip</p> <ul> <li>As in any k-mer based method, larger k-mers will provide more specificity, while smaller k-mers will provide more sensitivity. Larger genomes will also require larger k-mers to avoid k-mers that are shared by chance.</li> </ul> <p>:::</p>"},{"location":"usage/workflow/variant_and_refinement/#2-mapping-of-reads","title":"2. Mapping of reads","text":"<p>Mapping filtered reads to supercontig or mapping constraints is done with <code>BowTie2</code>,<code>BWAmem2</code> and <code>BWA</code>.</p> <p>The comparison of Bowtie2 and BWA-mem was done for Yao et al. (2020) where they found that BWA-MEM2 had a higher mapping rate (faster) and better accuracy. BWA-mem detected more variant bases in mapping reads than Bowtie2. The tool bwa-mem2 is the next version of the bwa-mem algorithm in bwa. It produces alignment identical to bwa and is ~1.3-3.1x faster depending on the use-case, dataset and the running machine.</p> <p>All three methods are supported to keep protocol compatibility with other pipelines and to allow the user to choose the best method for their data.</p> <p>The mapping tool can be specified with the <code>--mapper</code> parameter, the default is <code>bwamem2</code>, in case the intermediate mapper (for intermediate refinement cycles) needs to be different, this can be specified with <code>--intermediate_mapper</code> otherwise it uses the supplied <code>--mapper</code> tool.</p>"},{"location":"usage/workflow/variant_and_refinement/#21-deduplication","title":"2.1 Deduplication","text":"<p>Read deduplication is an optional step that can be performed with <code>Picard</code> or if UMIs are used <code>UMI-tools</code>. Unless you are using UMIs it is not possible to establish whether the fragments you have sequenced from your sample were derived via true biological duplication (i.e. sequencing independent template fragments) or as a result of PCR biases introduced during the library preparation. To correct your reads, use picard MarkDuplicates to mark the duplicate reads identified amongst the alignments to allow you to gauge the overall level of duplication in your samples. So if you have UMI\u2019s, no need to use Picard, instead use UMI-tools to deduplicate your reads. Where instead of mapping location and read similarity, UMI-tools uses the UMI to identify PCR duplicates.</p> <p>Specify <code>--deduplicate</code> to enable deduplication, the default is <code>true</code>. If UMIs are used, specify <code>--with_umi</code> and <code>--umi_deduplicate 'mapping' | 'both'</code> to enable UMI-tools deduplication. UMIs can be in the read header, if it is not in the header specify <code>--skip_umi_extract false</code>, the default is <code>true</code>.</p> <p>By default the UMIs are separated in the header by ':' (for <code>bcl2fastq</code> when demultiplexing) if this is different, specify with <code>arguments_umitools_extract '--umi-separator \"_\"'</code> and <code>arguments_umitools_dedup '--umi-separator \"_\"'</code> .</p>"},{"location":"usage/workflow/variant_and_refinement/#22-mapping-statistics","title":"2.2 Mapping statistics","text":"<p>Viralmetagenome uses multiple tools to get statistics on the variants and on the read mapping. These tools are:</p> <ul> <li><code>samtools flagstat</code> to get the number of reads that are mapped, unmapped, paired, etc.</li> <li><code>samtools idxstats</code> to get the number of reads that are mapped to each reference sequence.</li> <li><code>samtools stats</code> to collect statistics from BAM files and outputs in a text format.</li> <li><code>picard CollectMultipleMetrics</code> to collect multiple metrics from a BAM file.</li> <li><code>mosdepth</code> to calculate genome-wide sequencing coverage.</li> </ul> <p>There is a little overlap between the tools, but they all provide a different perspective on the mapping statistics.</p> <p>By default, all these tools are run, but they can be skipped with the argument <code>--mapping_stats false</code>. In case the intermediate mapping statistics (for intermediate refinement cycles) don't need to be determined set <code>--intermediate_mapping_stats false</code>.</p>"},{"location":"usage/workflow/variant_and_refinement/#3-variant-calling","title":"3. Variant calling","text":"<p>Variant calling is done with <code>BCFTools</code> and <code>iVar</code>, here a SNP will need to have at least a depth of 10 and a base quality of 20.</p> <p>BCFtools is a set of utilities that manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. iVar is a computational package that contains functions broadly useful for viral amplicon-based sequencing while each of iVar functions can be accomplished using existing tools, iVar contains an intersection of functionality from multiple tools that are required to call iSNVs and consensus sequences from viral sequencing data across multiple replicates.</p> <p>There are multiple studies on the benchmarking of variant callers as this is an area with active development. For instance Bassano et al. (2023) noticed that BCFtools called mutations with higher precision and recall than iVar. However, the reason behind this is that iVar has a lower precision than the others within their setup as it detects a lot of \u2018additional\u2019 variants within the sample, resulting in a higher amount of false positives but also true positives.</p> <p>:::tip Bcftools doesn't handle well multiallelic sites, so if you have a lot of multiallelic sites, iVar is the better choice. iVar is also the better choice if you have a lot of low-frequency variants. :::</p> <p>The variant caller can be specified with the <code>--variant_caller</code> parameter, the default is <code>ivar</code>. In case the intermediate variant caller (for intermediate refinement cycles) needs to be different, this can be specified with <code>--intermediate_variant_caller</code> otherwise it uses the supplied <code>--variant_caller</code> tool.</p>"},{"location":"usage/workflow/variant_and_refinement/#variant-filtering","title":"Variant filtering","text":"<p>The following steps are implemented for variant filtering.</p> <ul> <li>[only for <code>BCFtools</code>]: split up multiallelic sites into biallelic records and SNPs and indels should be merged into a single record.</li> <li>Variant filtering: filter out variants with an allelic depth of less than 75% of the average depth of the sample.</li> <li>[only for <code>iVar</code>]: strand bias correction &amp; collapsing variants belonging to the same codon.</li> </ul> <p>:::info If these filtering options are not to your liking, you can modify all of them. See the section on configuration for more information on how to do so. :::</p>"},{"location":"usage/workflow/variant_and_refinement/#4-consensus-calling","title":"4. Consensus calling","text":"<p>The consensus genome is updated with the variants of sufficient quality, either the ones determined previously in variant calling and filtering for the <code>--consensus_caller</code> <code>bcftools</code> or they are redetermined for <code>ivar</code>.</p> <p>There are again a couple of differences between the iVar and BCFtools:</p> <ol> <li>Low frequency deletions in iVar. <p>Areas of low frequency are more easily deleted and not carried along with iVar, this can be a bad thing during the iterative improvement of the consensus but is a good thing at the final consensus step.</p> </li> <li>Ambiguous nucleotides for multi-allelic sites in iVar. <p>iVar is capable to give lower frequency nucleotides ambiguous bases a summarising annotation instead of 'N'. For example at a certain position, the frequency of 'A' is 40% and of 'G' is 40%. Instead of reporting an 'N', iVar will report 'R'.</p> <p></p> </li> <li>Ambiguous nucleotides for low read depth. <p>In case of a low read depth at a certain position, if it doesn't get flagged by bcftools during variant calling, it will not be considered as a variant and the consensus will not be updated. iVar will update the consensus with an ambiguous base in case of low read depth.</p> <p></p> </li> </ol> <p>The consensus caller can be specified with the <code>--consensus_caller</code> parameter, the default is <code>ivar</code>. The intermediate consensus caller (for intermediate refinement cycles) can be specified with <code>--intermediate_consensus_caller</code> and is by default <code>bcftools</code>.</p>"},{"location":"workflow/consensus_qc/","title":"Report generation and quality control","text":"<p>Viralmetagenome's report and result interpretation heavily relies on MultiQC. MultiQC is a tool to create a single report from multiple analysis results. It is designed to be used with a wide range of bioinformatics tools and is compatible with a wide range of data formats. Almost all tools are summarised within the MultiQC report that have interactive plots and data tables. However, due to the number of tools included, some results are summarised in the directory <code>overview-tables</code> to reduce the size of the MultiQC report.</p> <p>Tip</p> <p>Complete output descriptions of files and images can be found in the output section.</p> <p>Within the MultiQC report, Viralmetagenome provides a number of custom tables based on consensus genome quality control data. These tools are:</p> <ul> <li>QUAST: QUAST is a quality assessment tool for genome assemblies. It calculates various metrics such as N50, L50, number of contigs, and total length.</li> <li>CheckV: CheckV is a tool for assessing the quality of metagenome-assembled viral genomes. It calculates various metrics such as completeness, contamination, and strain heterogeneity.</li> <li>Prokka: Prokka is a whole genome annotation pipeline for identifying features of interest in a set of genomic DNA sequences, and labelling them with useful information.</li> <li>blastn: BLAST is a tool for comparing primary biological sequence information. It calculates the similarity between the consensus genome and the reference genome.</li> <li>mmseqs-search - included as 'annotation': MMseqs is an ultra-fast and sensitive search tool for protein and nucleotide databases. Viralmetagenome uses MMseqs to annotate the consensus genomes and assign them a species name, segment name, expected host, etc.</li> <li>mafft: MAFFT is a multiple sequence alignment program.</li> <li>SnpEff and SnpSift: SnpEff is a genetic variant annotation and functional effect prediction tool. SnpSift is a toolbox that allows you to filter and manipulate annotated files.</li> </ul> <p>Consensus genome quality control can be skipped with <code>--skip_consensus_qc</code>.</p>"},{"location":"workflow/consensus_qc/#quast","title":"QUAST","text":"<p>QUAST is a quality assessment tool for genome assemblies. It calculates various metrics such as N50, L50, number of contigs, and total length. However, in the summary table, it is mainly used to get the number of ambiguous bases in the consensus genome.</p> <p>QUAST can be skipped with <code>--skip_quast</code>.</p>"},{"location":"workflow/consensus_qc/#checkv","title":"CheckV","text":"<p>CheckV is a tool for assessing the quality of metagenome-assembled viral genomes. It calculates various metrics such as completeness, contamination, and strain heterogeneity. CheckV estimates completeness by comparing sequences with a large database of complete viral genomes, metagenomes, metatranscriptomes, and metaviromes.</p> <p>Incomplete genomes for segmented viruses</p> <p>CheckV estimates the completeness of a virus based on all genome segments. If a virus has multiple segments, the completeness of the virus is calculated based on the length of the concatenated segments. For example, Lassa virus has 2 segments L: 7.2kb and S: 3.4kb. The completeness of the virus is calculated based on the length of the concatenated segments (7.2kb + 3.4kb = 10.6kb) and so if the generated consensus genome of the L segment is 7.1kb it will report the completeness as 7.1/10.6 ~ 67%.</p> <p>CheckV can be skipped with <code>--skip_checkv</code>.</p>"},{"location":"workflow/consensus_qc/#prokka","title":"Prokka","text":"<p>Prokka is a whole genome annotation pipeline for identifying features of interest in a set of genomic DNA sequences, and labelling them with useful information. Prokka is a software tool to annotate bacterial, archaeal and viral genomes.</p> <p>Suboptimal annotation</p> <p>Prokka was initially designed for bacterial and archaeal genomes, and may not be optimal for viral genomes. VIGOR4 is a good alternative but is species specific.</p> <p>Custom protein database</p> <p>Prokka can be given a custom protein database to annotate your genomes with, have a look at prot-RVDB for viral protein databases. Supply the database using <code>--prokka_db</code>.</p> <p>Prokka can be skipped with <code>--skip_prokka</code>.</p>"},{"location":"workflow/consensus_qc/#blast","title":"BLAST","text":"<p>blastn is a tool for comparing primary biological sequence information. It calculates the similarity between the consensus genome and the reference genome. The similarity is calculated based on the number of identical bases between the two sequences. Viralmetagenome uses blastn to compare the sequences against the supplied <code>--reference_pool</code> dataset.</p> <p>BLASTn can be skipped with <code>--skip_blast_qc</code>.</p>"},{"location":"workflow/consensus_qc/#mmseqs-search","title":"MMseqs-search","text":"<p>MMseqs-search is an ultra-fast and sensitive search tool for protein and nucleotide databases. Viralmetagenome uses MMseqs to search the consensus genomes in an annotated database, like Virousarus (see also defining your own custom annotation database), and uses the annotation data of the best hit to assign the consensus genome a species name, segment name, expected host, and any other metadata that is embedded within the database. This allows Viralmetagenome, in addition to the BLAST search of reference pool hits, to compare the generated consensus genomes at a species &amp; segment level.</p> <p>Info</p> <p>MMseqs was used for the annotation step instead of BLAST because of the ability to query using a tblastx search for highly diverging viruses while supplying a nucleotide annotation database. To specify another type of search (e.g. blastp, blastx, etc.), please refer to the parameters consensus-qc section.</p> <p>MMseqs-search can be skipped with <code>--skip_consensus_annotation</code>.</p>"},{"location":"workflow/consensus_qc/#snpeff-and-snpsift","title":"SnpEff and SnpSift","text":"<p>SnpEff is a genetic variant annotation and functional effect prediction tool. It annotates and predicts the effects of genetic variants on genes and proteins (such as amino acid changes).</p> <p>SnpSift is a toolbox that allows you to filter and manipulate annotated files. The ExtractFields tool is used to extract specific information from the annotated VCF files into a tabular format for easier analysis.</p> <p>Viralmetagenome uses SnpEff to annotate variants identified by the variant calling process with functional information, and SnpSift ExtractFields to extract key information from the annotated variants into a more accessible tabular format.</p> <p>The annotation process provides valuable information about the impact of variants, including:</p> <ul> <li>Whether variants are synonymous or non-synonymous</li> <li>Changes in amino acid sequences</li> <li>Potential impact severity (HIGH, MODERATE, LOW, MODIFIER)</li> <li>Gene and transcript information</li> </ul> <p>Variant annotation can be skipped with <code>--skip_vcf_annotation</code>.</p>"},{"location":"workflow/consensus_qc/#mafft","title":"MAFFT","text":"<p>MAFFT is a multiple sequence alignment program. It is used to align the following genomic data:</p> <ul> <li>The final consensus genome</li> <li>The identified reference genome from <code>--reference_pool</code></li> <li>The de novo contigs from each assembler (that constituted the final consensus genome)</li> <li>Each consensus genome from the iterative refinement steps.</li> </ul> <p>MAFFT can be skipped with <code>--skip_alignment_qc</code>.</p>"},{"location":"workflow/consensus_qc/#multiqc","title":"MultiQC","text":"<p>MultiQC is a tool to create a single report with interactive plots for multiple bioinformatics analyses across many samples.</p> <p></p> <p>Reports are generated by scanning given directories for recognised log files. These are parsed and a single HTML report is generated summarising the statistics for all logs found. MultiQC reports can describe multiple analysis steps and large numbers of samples within a single plot, and multiple analysis tools making it ideal for routine fast quality control.</p> <p>MultiQC is also used to generate the <code>overview-tables</code> as it extracts the additional data from various tools. The data that needs to be extracted can be modified with the argument <code>--custom_table_headers</code> where a yml file shows which tools need to be included in the summary table in addition to BLAST, CheckV, QUAST, and MMseqs (annotation).</p> custom_table_headers.yml<pre><code>tool:\n  - Tool subsection: # if applicable\n      - name_in_mqc_table: \"new name\"\n      - output_reads: \"deduplicated reads\"\n      - percent_passing_dedup: \"% passing dedup\"\n</code></pre>"}]}